{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE 257.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytFDV_Tc9E0X"
      },
      "source": [
        "# import libs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn as sk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchtext.legacy.data as data\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import jieba\n",
        "import os\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUcwHxhQPPI3"
      },
      "source": [
        "# Preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBPDKZBW9zww"
      },
      "source": [
        "# read dataset\n",
        "train = pd.read_csv('./train.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IML85A49zzW"
      },
      "source": [
        "test = pd.read_csv('./test.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDa-YIxM9z3r"
      },
      "source": [
        "unlabeled = pd.read_csv('./Unlabeled.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "ooxqBHC1Aeyl",
        "outputId": "ca105ddf-5c03-42bb-fbf6-ede218256e67"
      },
      "source": [
        "train.head(2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ofiicial Account Name</th>\n",
              "      <th>Title</th>\n",
              "      <th>News Url</th>\n",
              "      <th>Image Url</th>\n",
              "      <th>Report Content</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>环球人物</td>\n",
              "      <td>中国反腐风刮到阿根廷，这个美到让人瘫痪的女总统，因为8个本子摊上大事了</td>\n",
              "      <td>http://mp.weixin.qq.com/s?__biz=MTAzNDI4MDc2MQ...</td>\n",
              "      <td>http://mmbiz.qpic.cn/mmbiz_jpg/hpcO6kWnPm6cX3M...</td>\n",
              "      <td>内容不符</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>西湖之声</td>\n",
              "      <td>腾讯为《如懿传》道歉？这部3亿大剧上映第一天遭网友狂吐槽：愣是拍成村头恋曲...</td>\n",
              "      <td>http://mp.weixin.qq.com/s?__biz=MTA2Mjk0MTE2MA...</td>\n",
              "      <td>http://mmbiz.qpic.cn/mmbiz_jpg/vQCGoQzHAbaAXRr...</td>\n",
              "      <td>满口胡言</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Ofiicial Account Name  ... label\n",
              "0                  环球人物  ...     0\n",
              "1                  西湖之声  ...     0\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "4aPFZGANAhIv",
        "outputId": "abc0ea36-a35c-4b5a-9688-1e8aa24aa39e"
      },
      "source": [
        "test.head(2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ofiicial Account Name</th>\n",
              "      <th>Title</th>\n",
              "      <th>News Url</th>\n",
              "      <th>Image Url</th>\n",
              "      <th>Report Content</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>私家车第一广播</td>\n",
              "      <td>国务院宣布：生孩子有补助了！明年1月起实施，浙江属于这档！</td>\n",
              "      <td>http://mp.weixin.qq.com/s?__biz=MTA1NTc0MjE0MA...</td>\n",
              "      <td>http://mmbiz.qpic.cn/mmbiz_jpg/j27ttKHs7TlFAL5...</td>\n",
              "      <td>国务院没有发布过类似信息</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>杭州交通918</td>\n",
              "      <td>4个年轻帅小伙突然人没了, 身亡真相惊呆所有人! 太可惜了</td>\n",
              "      <td>http://mp.weixin.qq.com/s?__biz=MTA5Mzc3MDQyMA...</td>\n",
              "      <td>http://mmbiz.qpic.cn/mmbiz_jpg/0y9ibmULDTbDuCt...</td>\n",
              "      <td>？？？？</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Ofiicial Account Name                          Title  ... Report Content label\n",
              "0               私家车第一广播  国务院宣布：生孩子有补助了！明年1月起实施，浙江属于这档！  ...   国务院没有发布过类似信息     0\n",
              "1               杭州交通918  4个年轻帅小伙突然人没了, 身亡真相惊呆所有人! 太可惜了  ...           ？？？？     0\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "l4Cq2_ADAhWH",
        "outputId": "1ae2ab5e-eced-4586-b65d-b7fcd7f1e445"
      },
      "source": [
        "unlabeled.head(2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image Url</th>\n",
              "      <th>News Url</th>\n",
              "      <th>Ofiicial Account Name</th>\n",
              "      <th>Report Content</th>\n",
              "      <th>Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://mmbiz.qpic.cn/mmbiz_jpg/hNIfUeDqtnzpxX5...</td>\n",
              "      <td>http://mp.weixin.qq.com/s?__biz=MTAyNTI4NDgyMQ...</td>\n",
              "      <td>电子竞技</td>\n",
              "      <td>所属内容不实</td>\n",
              "      <td>直言不讳 | 为什么要包容RNG？</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://mmbiz.qpic.cn/mmbiz_jpg/pSEjsWXoC3qFM10...</td>\n",
              "      <td>http://mp.weixin.qq.com/s?__biz=MTAzMDM2MjI4MQ...</td>\n",
              "      <td>腾讯大秦网</td>\n",
              "      <td>欺诈</td>\n",
              "      <td>31省份最低工资排行出炉：上海2420最高，陕西是……</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Image Url  ...                        Title\n",
              "0  http://mmbiz.qpic.cn/mmbiz_jpg/hNIfUeDqtnzpxX5...  ...            直言不讳 | 为什么要包容RNG？\n",
              "1  http://mmbiz.qpic.cn/mmbiz_jpg/pSEjsWXoC3qFM10...  ...  31省份最低工资排行出炉：上海2420最高，陕西是……\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaEgM3UcAsvQ"
      },
      "source": [
        "# cut dfs to fit size of dfs in paper"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFqncckbPz2e",
        "outputId": "b94a1efd-52cf-4f46-85f4-3a2599dd3139"
      },
      "source": [
        "train_cut = pd.concat([train[train[\"label\"] == 0][:2000], train[train[\"label\"] == 1][:2000]])\n",
        "train_cut[\"label\"].value_counts()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    2000\n",
              "0    2000\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C51QnBmJQyBY",
        "outputId": "3b5cce08-98d2-4a6c-a949-53070f809f89"
      },
      "source": [
        "test_cut = pd.concat([test[test[\"label\"] == 0][:1600], test[test[\"label\"] == 1][:1400]])\n",
        "test_cut[\"label\"].value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1600\n",
              "1    1400\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN8cghGGQyIi",
        "outputId": "42fd04c3-ecaa-467f-93d6-08b2bbaf3475"
      },
      "source": [
        "unlabeled_cut = unlabeled[:30000]\n",
        "unlabeled_cut.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQb58BwHgMl3"
      },
      "source": [
        "# drop useless columns for feature extraction\n",
        "temp = train_cut[[\"Report Content\", \"label\"]]\n",
        "temp.to_csv(\"train_annotator.csv\", index = False)\n",
        "temp = test_cut[[\"Report Content\", \"label\"]]\n",
        "temp.to_csv(\"test_annotator.csv\", index = False)\n",
        "temp = unlabeled_cut[[\"Report Content\"]]\n",
        "temp.to_csv(\"unlabeled_annotator.csv\", index = False)\n",
        "\n",
        "# split dataset for supervised learning of neural nets\n",
        "temp = train_cut[[\"Title\", \"label\"]]\n",
        "temp.to_csv(\"train_supervise.csv\", index = False)\n",
        "temp = test_cut[[\"Title\", \"label\"]]\n",
        "temp.to_csv(\"test_supervise.csv\", index = False)\n",
        "# temp = unlabeled_cut[[\"Report Content\"]]\n",
        "# temp.to_csv(\"unlabeled_annotator.csv\", index = False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEfpfwMrXywV"
      },
      "source": [
        "# Logisitic annotator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE4s8J6VYe8f"
      },
      "source": [
        "# \n",
        "regex = re.compile(r'[^\\u4e00-\\u9fa5aA-Za-z0-9]')\n",
        "\n",
        "\n",
        "def word_cut(text):\n",
        "    text = regex.sub(' ', text)\n",
        "    return [word for word in jieba.cut(text) if word.strip()]\n",
        "\n",
        "\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"[0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EwMKNWmJzl-",
        "outputId": "a95b148c-d145-476e-9aad-38957d8989f8"
      },
      "source": [
        "train_cut[\"Report Content\"] + train_cut[\"Title\"]"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                 内容不符中国反腐风刮到阿根廷，这个美到让人瘫痪的女总统，因为8个本子摊上大事了\n",
              "1            满口胡言腾讯为《如懿传》道歉？这部3亿大剧上映第一天遭网友狂吐槽：愣是拍成村头恋曲...\n",
              "2                            ？ 顺风车司机奸杀20岁女乘客，落网视频曝光！滴滴道歉…\n",
              "3        领个屁证，过你妹的七夕，几天前的图在今天拿来博眼球偶遇鹿晗关晓彤旅行过七夕，小情侣是真滴甜...\n",
              "4                           事件不实。赵丽颖和冯绍峰即将公布恋情？网友：曝不曝没区别啊\n",
              "                              ...                        \n",
              "9839                                          不实倪萍大姐终于走了！\n",
              "9840      标题与内容不符##骗点击##欺骗阅读者。马容竟然去了非诚匆扰，一出场24盏灯全灭，孟爷爷都笑了\n",
              "9841             主题于内容不符，欺骗读者##信息不实杨幂证实已离婚独自带娃！刘恺威终于正面回应！\n",
              "9842                  内容##题目虚假41岁刘涛出轨选择离婚，震惊娱乐圈，她将何去何从...\n",
              "9843    标题不实，为了打广告##刘涛怎么外遇了，说那么多不就是想让大家买你那化妆品吗41岁刘涛外遇选...\n",
              "Length: 4000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39zyrsujXyTZ"
      },
      "source": [
        "train_cut_label = train_cut[\"label\"]\n",
        "train_cut_corpus = train_cut[\"Report Content\"]\n",
        "y_test = test_cut.label\n",
        "X_test = test_cut[\"Report Content\"]\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_cut_corpus, train_cut_label, test_size=0.2, random_state=42)"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb8GzmvaYWe9"
      },
      "source": [
        "logit_vec = TfidfVectorizer(tokenizer=word_cut)\n",
        "X_train_tfidf = logit_vec.fit_transform(X_train)\n",
        "X_val_tfidf = logit_vec.transform(X_val)\n",
        "X_test_tfidf = logit_vec.transform(X_test)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISLHjboYYNpP",
        "outputId": "af08fed8-ed08-411d-f23d-3043e9c17d23"
      },
      "source": [
        "logit_annotator = LogisticRegression()\n",
        "logit_annotator.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, logit_annotator.predict(X_test_tfidf), digits=4))"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6843    0.7113    0.6975      1600\n",
            "           1     0.6545    0.6250    0.6394      1400\n",
            "\n",
            "    accuracy                         0.6710      3000\n",
            "   macro avg     0.6694    0.6681    0.6685      3000\n",
            "weighted avg     0.6704    0.6710    0.6704      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkdlnWnIJrNP"
      },
      "source": [
        "# Logisitic annotator concat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLSQ5K01JN_7"
      },
      "source": [
        "train_cut_label = train_cut[\"label\"]\n",
        "train_cut_corpus = train_cut[\"Report Content\"] + train_cut[\"Title\"]\n",
        "y_test = test_cut.label\n",
        "X_test = test_cut[\"Report Content\"] + test_cut[\"Title\"]\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_cut_corpus, train_cut_label, test_size=0.2, random_state=42)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WthqT_70KTJY"
      },
      "source": [
        "logit_con = TfidfVectorizer(tokenizer=word_cut)\n",
        "X_train_tfidf = logit_con.fit_transform(X_train)\n",
        "X_val_tfidf = logit_con.transform(X_val)\n",
        "X_test_tfidf = logit_con.transform(X_test)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oS3no8FKTOw",
        "outputId": "fd740fec-8caf-4116-c10a-a98acb0a87c4"
      },
      "source": [
        "logit_concat = LogisticRegression()\n",
        "logit_concat.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, logit_concat.predict(X_test_tfidf), digits=4))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7265    0.8850    0.7980      1600\n",
            "           1     0.8249    0.6193    0.7075      1400\n",
            "\n",
            "    accuracy                         0.7610      3000\n",
            "   macro avg     0.7757    0.7521    0.7527      3000\n",
            "weighted avg     0.7724    0.7610    0.7557      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4c5FyfuW-Tj"
      },
      "source": [
        "# CNN feature extractor & Annotator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcFeftukSZaa"
      },
      "source": [
        "#Creating field for text and label\n",
        "TEXT = data.Field(sequential=True, tokenize=word_cut)\n",
        "LABEL = data.Field(sequential=False)\n",
        "\n",
        "# TEXT.preprocessing = data.Pipeline(clean_str)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF1Mm5-lbfiH"
      },
      "source": [
        "train_datafield = [('text', TEXT),  ('label', LABEL)]\n",
        "train_cnn = data.TabularDataset(path ='./train_annotator.csv',  \n",
        "                             format='csv',\n",
        "                             skip_header=True,\n",
        "                             fields=train_datafield)\n",
        "\n",
        "\n",
        "#%%\n",
        "test_datafield = [('text', TEXT),  ('label',LABEL)]\n",
        "\n",
        "test_cnn = data.TabularDataset(path ='./test_annotator.csv', \n",
        "                       format='csv',\n",
        "                       skip_header=True,\n",
        "                       fields=test_datafield)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8mpcHU5cYYI",
        "outputId": "3a96a827-5527-4ac0-831f-949251617260"
      },
      "source": [
        "train_cnn[0].text"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['内容', '不符']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wmymVW5yjqGS",
        "outputId": "a02b981e-a453-4cc4-c9ff-e59e222d7901"
      },
      "source": [
        "train_cnn[0].label"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDR7ojxBj1UR"
      },
      "source": [
        "# build vocab\n",
        "TEXT.build_vocab(train_cnn)\n",
        "LABEL.build_vocab(train_cnn)\n",
        "vocab = TEXT.vocab"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AFD1N4pkU9o",
        "outputId": "436aebc6-8cfe-4f8f-8a0e-e265df4914a0"
      },
      "source": [
        "TEXT.vocab.stoi['内容']"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuX_sHH9keE-"
      },
      "source": [
        "train_iter = data.Iterator(\n",
        "        train_cnn, \n",
        "        batch_size=64,\n",
        "        device=torch.device('cuda'), \n",
        "        sort_within_batch=False,\n",
        "        repeat=False)\n",
        "\n",
        "test_iter = data.Iterator(test_cnn, batch_size=64, device=torch.device('cuda'), \n",
        "                     sort_within_batch=False, repeat=False)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQNZVV79k_4L"
      },
      "source": [
        "# CNN module\n",
        "# output binary decision of given input report contents\n",
        "class textCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_built, emb_dim, dim_channel, kernel_wins, num_class):\n",
        "        super(textCNN, self).__init__()\n",
        "        #load pretrained embedding in embedding layer.\n",
        "        self.embed = nn.Embedding(len(vocab_built), emb_dim)\n",
        "        # self.embed.weight.data.copy_(vocab_built.vectors)\n",
        "    \n",
        "        #Convolutional Layers with different window size kernels\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n",
        "        #Dropout layer\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        \n",
        "        #FC layer\n",
        "        self.fc = nn.Linear(len(kernel_wins)*dim_channel, num_class)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        emb_x = self.embed(x)\n",
        "        emb_x = emb_x.unsqueeze(1)\n",
        "\n",
        "        con_x = [conv(emb_x) for conv in self.convs]\n",
        "\n",
        "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
        "        \n",
        "        fc_x = torch.cat(pool_x, dim=1)\n",
        "        \n",
        "        fc_x = fc_x.squeeze(-1)\n",
        "\n",
        "        fc_x = self.dropout(fc_x)\n",
        "        logit = self.fc(fc_x)\n",
        "        return logit"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auGVJZfalfaX"
      },
      "source": [
        "# train, eval for module\n",
        "def train(model, device, train_itr, optimizer, epoch, max_epoch):\n",
        "    model.train()\n",
        "    corrects, train_loss = 0.0,0\n",
        "    for batch in train_itr:\n",
        "        text, target = batch.text, batch.label\n",
        "        text = torch.transpose(text,0, 1)\n",
        "        target.data.sub_(1)\n",
        "        text, target = text.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logit = model(text)\n",
        "        \n",
        "        loss = F.cross_entropy(logit, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss+= loss.item()\n",
        "        result = torch.max(logit,1)[1]\n",
        "        corrects += (result.view(target.size()).data == target.data).sum()\n",
        "    \n",
        "    size = len(train_itr.dataset)\n",
        "    train_loss /= size \n",
        "    accuracy = 100.0 * corrects/size\n",
        "  \n",
        "    return train_loss, accuracy\n",
        "    \n",
        "def valid(model, device, test_itr):\n",
        "    model.eval()\n",
        "    corrects, test_loss = 0.0,0\n",
        "    fake_tp, fake_target_truth, fake_predicted_truth = 0, 0, 0\n",
        "    real_tp, real_target_truth, real_predicted_truth = 0, 0, 0\n",
        "    for batch in test_itr:\n",
        "        text, target = batch.text, batch.label\n",
        "        text = torch.transpose(text,0, 1)\n",
        "        target.data.sub_(1)\n",
        "        text, target = text.to(device), target.to(device)\n",
        "        logit = model(text)\n",
        "        loss = F.cross_entropy(logit, target)\n",
        "\n",
        "        \n",
        "        test_loss += loss.item()\n",
        "        # output prediction class with argmax\n",
        "        result = torch.max(logit,1)[1]\n",
        "        \n",
        "        # get tp, fp, fn of each batch\n",
        "        confusion_vector = result / target\n",
        "        fake_target_truth += torch.sum(result == 0).item()\n",
        "        real_target_truth += torch.sum(result == 1).item()\n",
        "        fake_predicted_truth += torch.sum(target == 0).item()\n",
        "        real_predicted_truth += torch.sum(target == 1).item()\n",
        "        fake_tp += torch.sum(torch.isnan(confusion_vector)).item()\n",
        "        real_tp += torch.sum(confusion_vector == 1).item()\n",
        "        corrects += (result.view(target.size()).data == target.data).sum()\n",
        "    size = len(test_itr.dataset)\n",
        "    test_loss /= size \n",
        "    accuracy = 100.0 * corrects/size\n",
        "\n",
        "    # calculate evaluation metrics\n",
        "    # fake news as tp\n",
        "    fake_recall = fake_tp/fake_target_truth\n",
        "    fake_precision = fake_tp/fake_predicted_truth\n",
        "    fake_f1 = 2 * fake_precision * fake_recall/(fake_precision + fake_recall)\n",
        "\n",
        "    # real news as tp\n",
        "    real_recall = real_tp/real_target_truth\n",
        "    real_precision = real_tp/real_predicted_truth\n",
        "    real_f1 = 2 * real_precision * real_recall/(real_precision + real_recall)\n",
        "    stat = {\n",
        "        'fake_recall': fake_recall,\n",
        "        'fake_precision': fake_precision,\n",
        "        'fake_f1': fake_f1,\n",
        "        'real_recall': real_recall,\n",
        "        'real_precision': real_precision,\n",
        "        'real_f1': real_f1,\n",
        "    }\n",
        "    \n",
        "    return test_loss, accuracy, stat"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcKOHOchk0uz",
        "outputId": "4be8e4c6-e045-4eb0-a78f-8022bd995fa9"
      },
      "source": [
        "# fine tuning CNN annotator\n",
        "annotator = textCNN(vocab, 128, 40, [1,2,3,4,5,6] , 2).to('cuda')\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "best_test_acc = -1\n",
        "\n",
        "# Use GPU if it is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#optimizer\n",
        "optimizer = optim.Adam(annotator.parameters(), lr=0.001)\n",
        "\n",
        "# fine tuning\n",
        "for epoch in range(1, 30):\n",
        "    #train loss\n",
        "    tr_loss, tr_acc = train(annotator, device, train_iter, optimizer, epoch, 100)\n",
        "    print('Train Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, tr_loss, tr_acc))\n",
        "    \n",
        "    ts_loss, ts_acc, stat = valid(annotator, device, test_iter)\n",
        "    print('Valid Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, ts_loss, ts_acc))\n",
        "    \n",
        "    if ts_acc > best_test_acc:\n",
        "        best_test_acc = ts_acc\n",
        "        print(stat)\n",
        "        #save paras(snapshot)\n",
        "        print(\"model saves at {} accuracy\".format(best_test_acc))\n",
        "        torch.save(annotator.state_dict(), \"textCNN_best\")\n",
        "        \n",
        "    train_loss.append(tr_loss)\n",
        "    train_acc.append(tr_acc)\n",
        "    test_loss.append(ts_loss)\n",
        "    test_acc.append(ts_acc)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 \t Loss: 0.011251639351248741 \t Accuracy: 58.72500228881836%\n",
            "Valid Epoch: 1 \t Loss: 0.010385637879371643 \t Accuracy: 61.79999923706055%\n",
            "{'fake_recall': 0.6603107344632768, 'fake_precision': 0.584375, 'fake_f1': 0.620026525198939, 'real_recall': 0.5801767676767676, 'real_precision': 0.6678571428571428, 'real_f1': 0.6209369715714471}\n",
            "model saves at 61.79999923706055 accuracy\n",
            "Train Epoch: 2 \t Loss: 0.009418256893754005 \t Accuracy: 68.1500015258789%\n",
            "Valid Epoch: 2 \t Loss: 0.010040067176024119 \t Accuracy: 65.43333435058594%\n",
            "{'fake_recall': 0.6749533871970168, 'fake_precision': 0.67875, 'fake_f1': 0.6768463695855407, 'real_recall': 0.6304816678648454, 'real_precision': 0.7757142857142857, 'real_f1': 0.6955981282678443}\n",
            "model saves at 65.43333435058594 accuracy\n",
            "Train Epoch: 3 \t Loss: 0.008765803605318069 \t Accuracy: 72.0%\n",
            "Valid Epoch: 3 \t Loss: 0.009958785692850749 \t Accuracy: 64.43333435058594%\n",
            "Train Epoch: 4 \t Loss: 0.008033710584044456 \t Accuracy: 74.92500305175781%\n",
            "Valid Epoch: 4 \t Loss: 0.010035070637861888 \t Accuracy: 65.4000015258789%\n",
            "Train Epoch: 5 \t Loss: 0.007456676989793777 \t Accuracy: 77.70000457763672%\n",
            "Valid Epoch: 5 \t Loss: 0.009825810422499975 \t Accuracy: 67.96666717529297%\n",
            "{'fake_recall': 0.6893894487255483, 'fake_precision': 0.726875, 'fake_f1': 0.7076361423790691, 'real_recall': 0.6671744097486672, 'real_precision': 0.8307142857142857, 'real_f1': 0.7400166847108994}\n",
            "model saves at 67.96666717529297 accuracy\n",
            "Train Epoch: 6 \t Loss: 0.006990737237036228 \t Accuracy: 78.95000457763672%\n",
            "Valid Epoch: 6 \t Loss: 0.010685085078080496 \t Accuracy: 66.0%\n",
            "Train Epoch: 7 \t Loss: 0.006720310486853123 \t Accuracy: 79.9000015258789%\n",
            "Valid Epoch: 7 \t Loss: 0.010741923451423645 \t Accuracy: 66.46666717529297%\n",
            "Train Epoch: 8 \t Loss: 0.006310339082032442 \t Accuracy: 81.9000015258789%\n",
            "Valid Epoch: 8 \t Loss: 0.01034254010518392 \t Accuracy: 67.03333282470703%\n",
            "Train Epoch: 9 \t Loss: 0.0058147100284695625 \t Accuracy: 83.45000457763672%\n",
            "Valid Epoch: 9 \t Loss: 0.010493639330069224 \t Accuracy: 66.5%\n",
            "Train Epoch: 10 \t Loss: 0.005303253687918186 \t Accuracy: 85.12500762939453%\n",
            "Valid Epoch: 10 \t Loss: 0.011131591906150182 \t Accuracy: 67.13333129882812%\n",
            "Train Epoch: 11 \t Loss: 0.005304580327123404 \t Accuracy: 84.80000305175781%\n",
            "Valid Epoch: 11 \t Loss: 0.011517875929673512 \t Accuracy: 67.4000015258789%\n",
            "Train Epoch: 12 \t Loss: 0.004742676597088575 \t Accuracy: 86.2750015258789%\n",
            "Valid Epoch: 12 \t Loss: 0.011201005876064301 \t Accuracy: 67.5%\n",
            "Train Epoch: 13 \t Loss: 0.00439478537812829 \t Accuracy: 88.22500610351562%\n",
            "Valid Epoch: 13 \t Loss: 0.012035113871097564 \t Accuracy: 67.26666259765625%\n",
            "Train Epoch: 14 \t Loss: 0.004898211086168885 \t Accuracy: 86.1500015258789%\n",
            "Valid Epoch: 14 \t Loss: 0.011878599345684051 \t Accuracy: 67.5%\n",
            "Train Epoch: 15 \t Loss: 0.004289251379668712 \t Accuracy: 88.17500305175781%\n",
            "Valid Epoch: 15 \t Loss: 0.012382770339647929 \t Accuracy: 66.5999984741211%\n",
            "Train Epoch: 16 \t Loss: 0.004154905993491411 \t Accuracy: 87.85000610351562%\n",
            "Valid Epoch: 16 \t Loss: 0.012180222928524017 \t Accuracy: 67.69999694824219%\n",
            "Train Epoch: 17 \t Loss: 0.004237540943548083 \t Accuracy: 88.55000305175781%\n",
            "Valid Epoch: 17 \t Loss: 0.012461633642514547 \t Accuracy: 66.96666717529297%\n",
            "Train Epoch: 18 \t Loss: 0.004021229131147265 \t Accuracy: 89.17500305175781%\n",
            "Valid Epoch: 18 \t Loss: 0.012110795736312865 \t Accuracy: 68.19999694824219%\n",
            "{'fake_recall': 0.705470737913486, 'fake_precision': 0.693125, 'fake_f1': 0.6992433795712484, 'real_recall': 0.6561624649859944, 'real_precision': 0.7921428571428571, 'real_f1': 0.7177691082428581}\n",
            "model saves at 68.19999694824219 accuracy\n",
            "Train Epoch: 19 \t Loss: 0.0036615747921168802 \t Accuracy: 90.00000762939453%\n",
            "Valid Epoch: 19 \t Loss: 0.012622549394766489 \t Accuracy: 67.56666564941406%\n",
            "Train Epoch: 20 \t Loss: 0.003739219758659601 \t Accuracy: 89.35000610351562%\n",
            "Valid Epoch: 20 \t Loss: 0.013393456717332204 \t Accuracy: 66.63333129882812%\n",
            "Train Epoch: 21 \t Loss: 0.0038558074235916138 \t Accuracy: 88.97500610351562%\n",
            "Valid Epoch: 21 \t Loss: 0.0165865132411321 \t Accuracy: 65.66666412353516%\n",
            "Train Epoch: 22 \t Loss: 0.003808808494359255 \t Accuracy: 89.80000305175781%\n",
            "Valid Epoch: 22 \t Loss: 0.013212408701578776 \t Accuracy: 67.4000015258789%\n",
            "Train Epoch: 23 \t Loss: 0.003465844502672553 \t Accuracy: 90.17500305175781%\n",
            "Valid Epoch: 23 \t Loss: 0.013605550428231557 \t Accuracy: 66.83333587646484%\n",
            "Train Epoch: 24 \t Loss: 0.0035982392225414515 \t Accuracy: 89.67500305175781%\n",
            "Valid Epoch: 24 \t Loss: 0.013614684204260508 \t Accuracy: 67.13333129882812%\n",
            "Train Epoch: 25 \t Loss: 0.0034762499537318945 \t Accuracy: 90.30000305175781%\n",
            "Valid Epoch: 25 \t Loss: 0.013847773631413778 \t Accuracy: 67.56666564941406%\n",
            "Train Epoch: 26 \t Loss: 0.0035877102576196192 \t Accuracy: 90.25000762939453%\n",
            "Valid Epoch: 26 \t Loss: 0.014031004071235656 \t Accuracy: 66.5%\n",
            "Train Epoch: 27 \t Loss: 0.00332626679725945 \t Accuracy: 90.30000305175781%\n",
            "Valid Epoch: 27 \t Loss: 0.013730567117532094 \t Accuracy: 67.36666870117188%\n",
            "Train Epoch: 28 \t Loss: 0.003430433066561818 \t Accuracy: 89.92500305175781%\n",
            "Valid Epoch: 28 \t Loss: 0.013789816240469615 \t Accuracy: 67.69999694824219%\n",
            "Train Epoch: 29 \t Loss: 0.0033755765939131378 \t Accuracy: 90.37500762939453%\n",
            "Valid Epoch: 29 \t Loss: 0.01354904427131017 \t Accuracy: 66.96666717529297%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMz9u9G4VNqq"
      },
      "source": [
        "# Supervised setting: Predict given labeled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ4l230dov4T"
      },
      "source": [
        "## split dataset & tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF_Eg1RKqL3V"
      },
      "source": [
        "train_cut_label = train_cut[\"label\"]\n",
        "train_cut_corpus = train_cut[\"Title\"]\n",
        "y_test = test_cut.label\n",
        "X_test = test_cut[\"Title\"]\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_cut_corpus, train_cut_label, test_size=0.2, random_state=42)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN0B9U2tUphQ",
        "outputId": "d09d4ca3-6de0-45e8-9198-3d15829c6833"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3200,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8FqaIGXUmff",
        "outputId": "295f18cb-4689-4d15-a81f-6fcb8b879bf2"
      },
      "source": [
        "X_val.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBe4fspUU0KA"
      },
      "source": [
        "# get tfidf\n",
        "vec = TfidfVectorizer(tokenizer=word_cut)\n",
        "X_train_tfidf = vec.fit_transform(X_train)\n",
        "X_val_tfidf = vec.transform(X_val)\n",
        "X_test_tfidf = vec.transform(X_test)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd0y_nnTosjw"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei-fanEXorkf",
        "outputId": "d1281064-3dd8-4ad3-9feb-8b0d4d6e280a"
      },
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, lr.predict(X_test_tfidf), digits=4))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6977    0.8912    0.7827      1600\n",
            "           1     0.8180    0.5586    0.6638      1400\n",
            "\n",
            "    accuracy                         0.7360      3000\n",
            "   macro avg     0.7578    0.7249    0.7232      3000\n",
            "weighted avg     0.7538    0.7360    0.7272      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNIrAfJIpcQB"
      },
      "source": [
        "## Linear SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9VaQkU9pg-q",
        "outputId": "eba37ebf-49a4-431b-d748-fc94b6e50327"
      },
      "source": [
        "svc = LinearSVC()\n",
        "svc.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, svc.predict(X_test_tfidf), digits=4))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7499    0.8844    0.8116      1600\n",
            "           1     0.8338    0.6629    0.7386      1400\n",
            "\n",
            "    accuracy                         0.7810      3000\n",
            "   macro avg     0.7918    0.7736    0.7751      3000\n",
            "weighted avg     0.7890    0.7810    0.7775      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gtsMaFkpeEd"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzVCUZx-phmA",
        "outputId": "81958889-c76e-4751-850c-b9d3e3c3c772"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, rf.predict(X_test_tfidf), digits=4))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7261    0.9113    0.8082      1600\n",
            "           1     0.8569    0.6071    0.7107      1400\n",
            "\n",
            "    accuracy                         0.7693      3000\n",
            "   macro avg     0.7915    0.7592    0.7595      3000\n",
            "weighted avg     0.7871    0.7693    0.7627      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIj76Lqma0cO"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2edP0s6da6pt"
      },
      "source": [
        "# prepare data for nn\n",
        "nn_text = data.Field(sequential=True, tokenize=word_cut)\n",
        "nn_label = data.Field(sequential=False)\n",
        "train_nn_datafield = [('text', nn_text),  ('label', nn_label)]\n",
        "test_nn_datafield = [('text', nn_text),  ('label', nn_label)]\n",
        "train_supervise = data.TabularDataset(path ='./train_supervise.csv',  \n",
        "                             format='csv',\n",
        "                             skip_header = True,\n",
        "                             fields = train_nn_datafield)\n",
        "test_supervise = data.TabularDataset(path ='./test_supervise.csv', \n",
        "                       format='csv',\n",
        "                       skip_header = True,\n",
        "                       fields=test_nn_datafield)\n",
        "nn_text.build_vocab(train_supervise)\n",
        "nn_label.build_vocab(train_supervise)\n",
        "nn_vocab = nn_text.vocab\n",
        "\n",
        "# set iterator for batch optimization\n",
        "train_iter = data.Iterator(\n",
        "        train_supervise, \n",
        "        batch_size=64,\n",
        "        device=torch.device('cuda'), \n",
        "        sort_within_batch=False,\n",
        "        repeat=False)\n",
        "\n",
        "test_iter = data.Iterator(test_supervise, batch_size=64, device=torch.device('cuda'), \n",
        "                     sort_within_batch=False, repeat=False)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn7pRk8ga6tU",
        "outputId": "9893c11e-2ab6-4db2-8734-cdccb62de744"
      },
      "source": [
        "# train CNN\n",
        "# fine tuning CNN model\n",
        "model = textCNN(nn_vocab, 200, 40, [1,2,3,4,5,6] , 2).to('cuda')\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "best_test_acc = -1\n",
        "\n",
        "# Use GPU if it is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# fine tuning\n",
        "for epoch in range(1, 30):\n",
        "    #train loss\n",
        "    tr_loss, tr_acc = train(model, device, train_iter, optimizer, epoch, 100)\n",
        "    print('Train Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, tr_loss, tr_acc))\n",
        "    \n",
        "    ts_loss, ts_acc, stat = valid(model, device, test_iter)\n",
        "    print('Valid Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, ts_loss, ts_acc))\n",
        "    \n",
        "    if ts_acc > best_test_acc:\n",
        "        best_test_acc = ts_acc\n",
        "        print(stat)\n",
        "        #save paras(snapshot)\n",
        "        print(\"model saves at {} accuracy\".format(best_test_acc))\n",
        "        torch.save(model.state_dict(), \"textCNN_supervise_best\")\n",
        "        \n",
        "    train_loss.append(tr_loss)\n",
        "    train_acc.append(tr_acc)\n",
        "    test_loss.append(ts_loss)\n",
        "    test_acc.append(ts_acc)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 \t Loss: 0.008621974140405655 \t Accuracy: 70.4000015258789%\n",
            "Valid Epoch: 1 \t Loss: 0.009188692440589268 \t Accuracy: 72.63333129882812%\n",
            "{'fake_recall': 0.6780978509373571, 'fake_precision': 0.926875, 'fake_f1': 0.7832057037232638, 'real_recall': 0.8560885608856088, 'real_precision': 0.49714285714285716, 'real_f1': 0.6290103931314958}\n",
            "model saves at 72.63333129882812 accuracy\n",
            "Train Epoch: 2 \t Loss: 0.004617636788636446 \t Accuracy: 87.92500305175781%\n",
            "Valid Epoch: 2 \t Loss: 0.010179349114497502 \t Accuracy: 75.23332977294922%\n",
            "{'fake_recall': 0.6932792061344158, 'fake_precision': 0.960625, 'fake_f1': 0.8053445113963846, 'real_recall': 0.9195402298850575, 'real_precision': 0.5142857142857142, 'real_f1': 0.6596426935409986}\n",
            "model saves at 75.23332977294922 accuracy\n",
            "Train Epoch: 3 \t Loss: 0.002801218992099166 \t Accuracy: 93.32500457763672%\n",
            "Valid Epoch: 3 \t Loss: 0.01182908237973849 \t Accuracy: 76.46666717529297%\n",
            "{'fake_recall': 0.7073283858998145, 'fake_precision': 0.953125, 'fake_f1': 0.8120340788072418, 'real_recall': 0.9111374407582938, 'real_precision': 0.5492857142857143, 'real_f1': 0.6853832442067735}\n",
            "model saves at 76.46666717529297 accuracy\n",
            "Train Epoch: 4 \t Loss: 0.0019840211304835973 \t Accuracy: 95.50000762939453%\n",
            "Valid Epoch: 4 \t Loss: 0.0131208516061306 \t Accuracy: 76.43333435058594%\n",
            "Train Epoch: 5 \t Loss: 0.0014502586154267192 \t Accuracy: 97.35000610351562%\n",
            "Valid Epoch: 5 \t Loss: 0.015934665888547896 \t Accuracy: 76.06666564941406%\n",
            "Train Epoch: 6 \t Loss: 0.0012242766977287828 \t Accuracy: 97.72500610351562%\n",
            "Valid Epoch: 6 \t Loss: 0.016463143666585286 \t Accuracy: 75.79999542236328%\n",
            "Train Epoch: 7 \t Loss: 0.0010126074503641576 \t Accuracy: 98.07500457763672%\n",
            "Valid Epoch: 7 \t Loss: 0.018243032236893972 \t Accuracy: 76.13333129882812%\n",
            "Train Epoch: 8 \t Loss: 0.0011124736820347608 \t Accuracy: 98.1500015258789%\n",
            "Valid Epoch: 8 \t Loss: 0.020023646553357442 \t Accuracy: 76.06666564941406%\n",
            "Train Epoch: 9 \t Loss: 0.0008235536081483588 \t Accuracy: 98.6500015258789%\n",
            "Valid Epoch: 9 \t Loss: 0.017696693976720173 \t Accuracy: 76.23332977294922%\n",
            "Train Epoch: 10 \t Loss: 0.0007814278118312359 \t Accuracy: 98.6500015258789%\n",
            "Valid Epoch: 10 \t Loss: 0.018678555130958556 \t Accuracy: 76.23332977294922%\n",
            "Train Epoch: 11 \t Loss: 0.0007888158760033548 \t Accuracy: 98.80000305175781%\n",
            "Valid Epoch: 11 \t Loss: 0.020356345216433206 \t Accuracy: 76.43333435058594%\n",
            "Train Epoch: 12 \t Loss: 0.0008240332474233582 \t Accuracy: 98.9000015258789%\n",
            "Valid Epoch: 12 \t Loss: 0.02066043120622635 \t Accuracy: 76.33333587646484%\n",
            "Train Epoch: 13 \t Loss: 0.0006939985186327249 \t Accuracy: 99.17500305175781%\n",
            "Valid Epoch: 13 \t Loss: 0.02452084438006083 \t Accuracy: 75.93333435058594%\n",
            "Train Epoch: 14 \t Loss: 0.0007625730590661988 \t Accuracy: 98.82500457763672%\n",
            "Valid Epoch: 14 \t Loss: 0.023506658126910527 \t Accuracy: 75.69999694824219%\n",
            "Train Epoch: 15 \t Loss: 0.0008082951303222217 \t Accuracy: 98.9000015258789%\n",
            "Valid Epoch: 15 \t Loss: 0.022657577057679495 \t Accuracy: 76.26666259765625%\n",
            "Train Epoch: 16 \t Loss: 0.0006333653365727514 \t Accuracy: 99.12500762939453%\n",
            "Valid Epoch: 16 \t Loss: 0.020640992244084676 \t Accuracy: 76.36666870117188%\n",
            "Train Epoch: 17 \t Loss: 0.0009875284297741018 \t Accuracy: 99.10000610351562%\n",
            "Valid Epoch: 17 \t Loss: 0.021644686698913573 \t Accuracy: 76.29999542236328%\n",
            "Train Epoch: 18 \t Loss: 0.0008464820820372552 \t Accuracy: 98.95000457763672%\n",
            "Valid Epoch: 18 \t Loss: 0.021635910511016845 \t Accuracy: 76.36666870117188%\n",
            "Train Epoch: 19 \t Loss: 0.000771593026118353 \t Accuracy: 98.97500610351562%\n",
            "Valid Epoch: 19 \t Loss: 0.0210112673441569 \t Accuracy: 76.16666412353516%\n",
            "Train Epoch: 20 \t Loss: 0.0009386498008389026 \t Accuracy: 99.0250015258789%\n",
            "Valid Epoch: 20 \t Loss: 0.01779580569267273 \t Accuracy: 76.36666870117188%\n",
            "Train Epoch: 21 \t Loss: 0.0008481520899513271 \t Accuracy: 99.17500305175781%\n",
            "Valid Epoch: 21 \t Loss: 0.01826183631022771 \t Accuracy: 76.63333129882812%\n",
            "{'fake_recall': 0.7137422729434142, 'fake_precision': 0.938125, 'fake_f1': 0.8106940318660546, 'real_recall': 0.8896321070234113, 'real_precision': 0.57, 'real_f1': 0.694819329560296}\n",
            "model saves at 76.63333129882812 accuracy\n",
            "Train Epoch: 22 \t Loss: 0.0005413388491142541 \t Accuracy: 99.20000457763672%\n",
            "Valid Epoch: 22 \t Loss: 0.021377827107906342 \t Accuracy: 76.19999694824219%\n",
            "Train Epoch: 23 \t Loss: 0.0006573227799381129 \t Accuracy: 99.25000762939453%\n",
            "Valid Epoch: 23 \t Loss: 0.022982894082864126 \t Accuracy: 76.06666564941406%\n",
            "Train Epoch: 24 \t Loss: 0.0006149977378663607 \t Accuracy: 99.2750015258789%\n",
            "Valid Epoch: 24 \t Loss: 0.020511058469613392 \t Accuracy: 76.29999542236328%\n",
            "Train Epoch: 25 \t Loss: 0.0007420536950594396 \t Accuracy: 99.22500610351562%\n",
            "Valid Epoch: 25 \t Loss: 0.020891179700692496 \t Accuracy: 76.33333587646484%\n",
            "Train Epoch: 26 \t Loss: 0.0007776491232216359 \t Accuracy: 99.10000610351562%\n",
            "Valid Epoch: 26 \t Loss: 0.020928852319717407 \t Accuracy: 75.9000015258789%\n",
            "Train Epoch: 27 \t Loss: 0.0005858518567110878 \t Accuracy: 99.17500305175781%\n",
            "Valid Epoch: 27 \t Loss: 0.02003214704990387 \t Accuracy: 76.56666564941406%\n",
            "Train Epoch: 28 \t Loss: 0.0005881022525136359 \t Accuracy: 99.12500762939453%\n",
            "Valid Epoch: 28 \t Loss: 0.01976251635948817 \t Accuracy: 76.56666564941406%\n",
            "Train Epoch: 29 \t Loss: 0.0005715277695271652 \t Accuracy: 99.37500762939453%\n",
            "Valid Epoch: 29 \t Loss: 0.01994422596693039 \t Accuracy: 76.46666717529297%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1LctNXBpiOB"
      },
      "source": [
        "# Weekly supervised:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0LGqiHCkHMS"
      },
      "source": [
        "## Using previously trained annotator to generate weak label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7j9YCwYDzEa"
      },
      "source": [
        "# generate weak label using annotator\n",
        "def predict(model, device, unlabeled):\n",
        "    model.eval()\n",
        "    for index, row in unlabeled.iterrows():\n",
        "        text = TEXT.preprocess(row['Report Content'])\n",
        "        text = [[TEXT.vocab.stoi[x] for x in text]]\n",
        "        if len(text[0]) <= 6:\n",
        "            for j in range(6 - len(text[0])):\n",
        "                text[0].append(1)\n",
        "        text = torch.Tensor(text).long()\n",
        "        # text = torch.transpose(text, 0, 1)\n",
        "        text = text.to(device)\n",
        "\n",
        "        # target.data.sub_(1)\n",
        "        logit = model(text)\n",
        "\n",
        "        # output prediction class with argmax\n",
        "        result = torch.max(logit,1)[1]\n",
        "\n",
        "        # assign label to row\n",
        "        unlabeled.loc[index, \"weak label\"] = result[0].item()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ejvfu_GenB0"
      },
      "source": [
        "unlabeled[\"weak label\"] = 0"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmGwUTWnf3CK"
      },
      "source": [
        "predict(annotator, device, unlabeled)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "id": "tkXt56hHiqWe",
        "outputId": "1f82e442-6fb3-4d11-82b9-202bde1440b4"
      },
      "source": [
        "unlabeled.head()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image Url</th>\n",
              "      <th>News Url</th>\n",
              "      <th>Ofiicial Account Name</th>\n",
              "      <th>Report Content</th>\n",
              "      <th>Title</th>\n",
              "      <th>weak label</th>\n",
              "      <th>lr label</th>\n",
              "      <th>lr prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://mmbiz.qpic.cn/mmbiz_jpg/hNIfUeDqtnzpxX5...</td>\n",
              "      <td>http://mp.weixin.qq.com/s?__biz=MTAyNTI4NDgyMQ...</td>\n",
              "      <td>电子竞技</td>\n",
              "      <td>所属内容不实</td>\n",
              "      <td>直言不讳 | 为什么要包容RNG？</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.531563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://mmbiz.qpic.cn/mmbiz_jpg/pSEjsWXoC3qFM10...</td>\n",
              "      <td>http://mp.weixin.qq.com/s?__biz=MTAzMDM2MjI4MQ...</td>\n",
              "      <td>腾讯大秦网</td>\n",
              "      <td>欺诈</td>\n",
              "      <td>31省份最低工资排行出炉：上海2420最高，陕西是……</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.680933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://mmbiz.qpic.cn/mmbiz_jpg/pSEjsWXoC3pia6u...</td>\n",
              "      <td>http://mp.weixin.qq.com/s?__biz=MTAzMDM2MjI4MQ...</td>\n",
              "      <td>腾讯大秦网</td>\n",
              "      <td>清谷田园并未使用青岛工厂提供的原料##该品牌果汁的原料跟发生烂苹果事件的工厂无任何关系。##...</td>\n",
              "      <td>可怕！国产果汁潜规则曝光，2毛一斤腐烂果被加工成高端果汁！</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.685111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://mmbiz.qpic.cn/mmbiz_jpg/pSEjsWXoC3pia6u...</td>\n",
              "      <td>http://mp.weixin.qq.com/s?__biz=MTAzMDM2MjI4MQ...</td>\n",
              "      <td>腾讯大秦网</td>\n",
              "      <td>鱼化寨要拆了不实信息</td>\n",
              "      <td>鱼化寨要拆了？再见了，西安“小香港”？</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.520913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://mmbiz.qpic.cn/mmbiz_jpg/2EVtnKem0SUGprk...</td>\n",
              "      <td>http://mp.weixin.qq.com/s?__biz=MTA0MzM2MTc4MQ...</td>\n",
              "      <td>不弄头发就闹心</td>\n",
              "      <td>我是云南人，没听过这种陋习</td>\n",
              "      <td>云南摸-奶节真实体验 场面不忍直视</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.617848</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Image Url  ...   lr prob\n",
              "0  http://mmbiz.qpic.cn/mmbiz_jpg/hNIfUeDqtnzpxX5...  ...  0.531563\n",
              "1  http://mmbiz.qpic.cn/mmbiz_jpg/pSEjsWXoC3qFM10...  ...  0.680933\n",
              "2  http://mmbiz.qpic.cn/mmbiz_jpg/pSEjsWXoC3pia6u...  ...  0.685111\n",
              "3  http://mmbiz.qpic.cn/mmbiz_jpg/pSEjsWXoC3pia6u...  ...  0.520913\n",
              "4  http://mmbiz.qpic.cn/mmbiz_jpg/2EVtnKem0SUGprk...  ...  0.617848\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzUMbNufLxvT"
      },
      "source": [
        "## label with concat annotator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzC7EUhLLWy7",
        "outputId": "bbd4ad72-31ea-4047-c2c9-d6eff8c1a6ab"
      },
      "source": [
        "unlabeled_tfidf = logit_con.transform(unlabeled_cut[\"Report Content\"] + unlabeled_cut[\"Title\"])\n",
        "prediction = logit_concat.predict(unlabeled_tfidf)\n",
        "unlabeled_con = unlabeled_cut\n",
        "unlabeled_con[\"con label\"] = prediction"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FHxyijRLXEk"
      },
      "source": [
        "unlabeled_label = unlabeled_con[\"con label\"]\n",
        "unlabeled_title = unlabeled_con[\"Title\"]\n",
        "train_label = train_cut[\"label\"]\n",
        "train_title = train_cut[\"Title\"]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPxkd294M2zH"
      },
      "source": [
        "train_weak_label = unlabeled_label\n",
        "train_weak_corpus = unlabeled_title\n",
        "y_test = test_cut.label\n",
        "X_test = test_cut[\"Report Content\"] + test_cut[\"Title\"]\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_weak_corpus, train_weak_label, test_size=0.2, random_state=42)"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umawwG3yM26h"
      },
      "source": [
        "# get tfidf\n",
        "vec = TfidfVectorizer(tokenizer=word_cut)\n",
        "X_train_tfidf = vec.fit_transform(X_train)\n",
        "X_val_tfidf = vec.transform(X_val)\n",
        "X_test_tfidf = vec.transform(X_test)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-fM_z5NM29n",
        "outputId": "d14fb5fa-0f64-40c0-8066-6c8291090071"
      },
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, lr.predict(X_test_tfidf), digits = 4))"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6196    0.9744    0.7575      1600\n",
            "           1     0.9153    0.3164    0.4703      1400\n",
            "\n",
            "    accuracy                         0.6673      3000\n",
            "   macro avg     0.7675    0.6454    0.6139      3000\n",
            "weighted avg     0.7576    0.6673    0.6235      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqNilL6xNJEW",
        "outputId": "09010a03-f29f-4c78-9b3f-396b72cc55c2"
      },
      "source": [
        "svc = LinearSVC()\n",
        "svc.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, svc.predict(X_test_tfidf), digits = 4))"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6514    0.9425    0.7704      1600\n",
            "           1     0.8657    0.4236    0.5688      1400\n",
            "\n",
            "    accuracy                         0.7003      3000\n",
            "   macro avg     0.7585    0.6830    0.6696      3000\n",
            "weighted avg     0.7514    0.7003    0.6763      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bGiS_EZNJLN",
        "outputId": "7160b94f-f963-42bc-977a-7cb428d2563e"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, rf.predict(X_test_tfidf), digits = 4))"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6185    0.9637    0.7535      1600\n",
            "           1     0.8856    0.3207    0.4709      1400\n",
            "\n",
            "    accuracy                         0.6637      3000\n",
            "   macro avg     0.7521    0.6422    0.6122      3000\n",
            "weighted avg     0.7432    0.6637    0.6216      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8LQodfXZdih"
      },
      "source": [
        "## label with logit annotator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auOpGtHzZc3U"
      },
      "source": [
        "unlabeled_tfidf = logit_vec.transform(unlabeled[\"Report Content\"])\n",
        "prediction = logit_annotator.predict(unlabeled_tfidf)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6dC-TVRc3tG"
      },
      "source": [
        "prob = logit_annotator.predict_proba(unlabeled_tfidf)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNSbeRF9Zl-7"
      },
      "source": [
        "unlabeled_lr = unlabeled"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOKeazHtZmB7"
      },
      "source": [
        "unlabeled_lr['lr label'] = prediction\n",
        "unlabeled_lr['lr prob'] = np.amax(prob, axis = 1)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VsXS6KWkONZ"
      },
      "source": [
        "## Perform detection on train and unlabeled dataset combined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9R8TCbkJtV6"
      },
      "source": [
        "### With CNN annotator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSVIMxjLrn8Z"
      },
      "source": [
        "unlabeled_label = unlabeled[\"weak label\"][:30000]\n",
        "unlabeled_title = unlabeled[\"Title\"][:30000]\n",
        "train_label = train_cut[\"label\"]\n",
        "train_title = train_cut[\"Title\"]"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7kng9Fakc3p"
      },
      "source": [
        "# split training and testing set\n",
        "# train_weak_label = pd.concat([train_label, unlabeled_label], ignore_index=True)\n",
        "# train_weak_corpus = pd.concat([train_title, unlabeled_title], ignore_index=True)\n",
        "train_weak_label = unlabeled_label\n",
        "train_weak_corpus = unlabeled_title\n",
        "y_test = test_cut.label\n",
        "X_test = test_cut[\"Title\"]\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_weak_corpus, train_weak_label, test_size=0.2, random_state=42)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxa0GbyfnxAP",
        "outputId": "e10e57f9-b3f5-4d60-b147-f96696f1caf6"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEk4nx2hn-qk"
      },
      "source": [
        "# get tfidf\n",
        "vec = TfidfVectorizer(tokenizer=word_cut)\n",
        "X_train_tfidf = vec.fit_transform(X_train)\n",
        "X_val_tfidf = vec.transform(X_val)\n",
        "X_test_tfidf = vec.transform(X_test)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOgO4LTsoPX-",
        "outputId": "d2acc7b2-fbfc-4b67-e4f9-e6b4f83828e3"
      },
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, lr.predict(X_test_tfidf), digits=4))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5911    0.9531    0.7297      1600\n",
            "           1     0.8214    0.2464    0.3791      1400\n",
            "\n",
            "    accuracy                         0.6233      3000\n",
            "   macro avg     0.7063    0.5998    0.5544      3000\n",
            "weighted avg     0.6986    0.6233    0.5661      3000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idJo6RHnoPe3",
        "outputId": "d742cef2-7a3c-4959-a125-520d14fba658"
      },
      "source": [
        "svc = LinearSVC()\n",
        "svc.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, svc.predict(X_test_tfidf), digits=4))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6257    0.8369    0.7160      1600\n",
            "           1     0.6965    0.4279    0.5301      1400\n",
            "\n",
            "    accuracy                         0.6460      3000\n",
            "   macro avg     0.6611    0.6324    0.6231      3000\n",
            "weighted avg     0.6587    0.6460    0.6293      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db231vQcoPhu",
        "outputId": "d4eed88c-c504-4f46-c438-744134b5453b"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, rf.predict(X_test_tfidf), digits=4))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6094    0.8794    0.7199      1600\n",
            "           1     0.7207    0.3557    0.4763      1400\n",
            "\n",
            "    accuracy                         0.6350      3000\n",
            "   macro avg     0.6650    0.6175    0.5981      3000\n",
            "weighted avg     0.6613    0.6350    0.6062      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28_WDp6ruYBx"
      },
      "source": [
        "# convert weak supervised training data to csv file\n",
        "train_weak_cnn = pd.DataFrame(columns=[\"Title\", \"label\"])\n",
        "train_weak_cnn[\"Title\"] = train_weak_corpus\n",
        "train_weak_cnn[\"label\"] = train_weak_label\n",
        "train_weak_cnn.to_csv(\"./train_weak.csv\", index = False)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2CGeDzSoPrm"
      },
      "source": [
        "# prepare data for nn\n",
        "nn_text = data.Field(sequential=True, tokenize=word_cut)\n",
        "nn_label = data.Field(sequential=False)\n",
        "train_nn_datafield = [('text', nn_text),  ('label', nn_label)]\n",
        "test_nn_datafield = [('text', nn_text),  ('label', nn_label)]\n",
        "train_supervise = data.TabularDataset(path ='./train_weak.csv',  \n",
        "                             format='csv',\n",
        "                             skip_header = True,\n",
        "                             fields = train_nn_datafield)\n",
        "test_supervise = data.TabularDataset(path ='./test_supervise.csv', \n",
        "                       format='csv',\n",
        "                       skip_header = True,\n",
        "                       fields=test_nn_datafield)\n",
        "nn_text.build_vocab(train_supervise)\n",
        "nn_label.build_vocab(train_supervise)\n",
        "nn_vocab = nn_text.vocab\n",
        "\n",
        "# set iterator for batch optimization\n",
        "train_iter = data.Iterator(\n",
        "        train_supervise, \n",
        "        batch_size=64,\n",
        "        device=torch.device('cuda'), \n",
        "        sort_within_batch=False,\n",
        "        repeat=False)\n",
        "\n",
        "test_iter = data.Iterator(test_supervise, batch_size=64, device=torch.device('cuda'), \n",
        "                     sort_within_batch=False, repeat=False)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x0a56d6vwUM"
      },
      "source": [
        "# train CNN\n",
        "# fine tuning CNN model\n",
        "model = textCNN(nn_vocab, 200, 40, [1,2,3,4,5,6] , 2).to('cuda')\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "best_test_acc = -1\n",
        "\n",
        "# Use GPU if it is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# fine tuning\n",
        "for epoch in range(1, 30):\n",
        "    #train loss\n",
        "    tr_loss, tr_acc = train(model, device, train_iter, optimizer, epoch, 100)\n",
        "    print('Train Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, tr_loss, tr_acc))\n",
        "    \n",
        "    ts_loss, ts_acc, stat = valid(model, device, test_iter)\n",
        "    print('Valid Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, ts_loss, ts_acc))\n",
        "    \n",
        "    if ts_acc > best_test_acc:\n",
        "        best_test_acc = ts_acc\n",
        "        print(stat)\n",
        "        #save paras(snapshot)\n",
        "        print(\"model saves at {} accuracy\".format(best_test_acc))\n",
        "        torch.save(model.state_dict(), \"textCNN_supervise_best\")\n",
        "        \n",
        "    train_loss.append(tr_loss)\n",
        "    train_acc.append(tr_acc)\n",
        "    test_loss.append(ts_loss)\n",
        "    test_acc.append(ts_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqvWkQSqJ2up"
      },
      "source": [
        "### With LR annotator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7osiSvnVJ6E-"
      },
      "source": [
        "unlabeled_label = unlabeled[\"lr label\"][:30000]\n",
        "unlabeled_title = unlabeled[\"Title\"][:30000]\n",
        "train_label = train_cut[\"label\"]\n",
        "train_title = train_cut[\"Title\"]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmMkwKWjJ6Hi"
      },
      "source": [
        "# split training and testing set\n",
        "# train_weak_label = pd.concat([train_label, unlabeled_label], ignore_index=True)\n",
        "# train_weak_corpus = pd.concat([train_title, unlabeled_title], ignore_index=True)\n",
        "train_weak_label = unlabeled_label\n",
        "train_weak_corpus = unlabeled_title\n",
        "y_test = test_cut.label\n",
        "X_test = test_cut[\"Title\"]\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_weak_corpus, train_weak_label, test_size=0.2, random_state=42)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uih4Bk8dJ6Ld"
      },
      "source": [
        "# get tfidf\n",
        "vec = TfidfVectorizer(tokenizer=word_cut)\n",
        "X_train_tfidf = vec.fit_transform(X_train)\n",
        "X_val_tfidf = vec.transform(X_val)\n",
        "X_test_tfidf = vec.transform(X_test)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3WiPBEEJ187",
        "outputId": "f61bf0df-2310-4e7b-a471-1e87e2503cd4"
      },
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, lr.predict(X_test_tfidf), digits = 4))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6506    0.9087    0.7583      1600\n",
            "           1     0.8092    0.4421    0.5718      1400\n",
            "\n",
            "    accuracy                         0.6910      3000\n",
            "   macro avg     0.7299    0.6754    0.6651      3000\n",
            "weighted avg     0.7246    0.6910    0.6713      3000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzPfcEpEKVH_",
        "outputId": "5c03530c-cc78-47c1-be3e-4ca7b92d5f0c"
      },
      "source": [
        "svc = LinearSVC()\n",
        "svc.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, svc.predict(X_test_tfidf), digits = 4))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6630    0.7819    0.7175      1600\n",
            "           1     0.6864    0.5457    0.6080      1400\n",
            "\n",
            "    accuracy                         0.6717      3000\n",
            "   macro avg     0.6747    0.6638    0.6628      3000\n",
            "weighted avg     0.6739    0.6717    0.6664      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndjxk0QjKZ63",
        "outputId": "7b70f0cf-01e3-4ab9-9976-b57fef461a3f"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_test, rf.predict(X_test_tfidf), digits = 4))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6346    0.8694    0.7336      1600\n",
            "           1     0.7413    0.4279    0.5426      1400\n",
            "\n",
            "    accuracy                         0.6633      3000\n",
            "   macro avg     0.6880    0.6486    0.6381      3000\n",
            "weighted avg     0.6844    0.6633    0.6445      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn-Nf5Uc-veA"
      },
      "source": [
        "# convert weak supervised training data to csv file\n",
        "train_weak_cnn = pd.DataFrame(columns=[\"Title\", \"label\"])\n",
        "train_weak_cnn[\"Title\"] = train_weak_corpus\n",
        "train_weak_cnn[\"label\"] = train_weak_label\n",
        "train_weak_cnn.to_csv(\"./train_weak.csv\", index = False)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "118akv5b_QPz"
      },
      "source": [
        "# prepare data for nn\n",
        "nn_text = data.Field(sequential=True, tokenize=word_cut)\n",
        "nn_label = data.Field(sequential=False)\n",
        "train_nn_datafield = [('text', nn_text),  ('label', nn_label)]\n",
        "test_nn_datafield = [('text', nn_text),  ('label', nn_label)]\n",
        "train_supervise = data.TabularDataset(path ='./train_weak.csv',  \n",
        "                             format='csv',\n",
        "                             skip_header = True,\n",
        "                             fields = train_nn_datafield)\n",
        "test_supervise = data.TabularDataset(path ='./test_supervise.csv', \n",
        "                       format='csv',\n",
        "                       skip_header = True,\n",
        "                       fields=test_nn_datafield)\n",
        "nn_text.build_vocab(train_supervise)\n",
        "nn_label.build_vocab(train_supervise)\n",
        "nn_vocab = nn_text.vocab\n",
        "\n",
        "# set iterator for batch optimization\n",
        "train_iter = data.Iterator(\n",
        "        train_supervise, \n",
        "        batch_size=64,\n",
        "        device=torch.device('cuda'), \n",
        "        sort_within_batch=False,\n",
        "        repeat=False)\n",
        "\n",
        "test_iter = data.Iterator(test_supervise, batch_size=64, device=torch.device('cuda'), \n",
        "                     sort_within_batch=False, repeat=False)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlV_zUV0_VZG",
        "outputId": "1bcf8855-f3b0-415a-a63b-e12c91ef0650"
      },
      "source": [
        "# train CNN\n",
        "# fine tuning CNN model\n",
        "model = textCNN(nn_vocab, 200, 40, [1,2,3,4,5,6] , 2).to('cuda')\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "best_test_acc = -1\n",
        "\n",
        "# Use GPU if it is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# fine tuning\n",
        "for epoch in range(1, 30):\n",
        "    #train loss\n",
        "    tr_loss, tr_acc = train(model, device, train_iter, optimizer, epoch, 100)\n",
        "    print('Train Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, tr_loss, tr_acc))\n",
        "    \n",
        "    ts_loss, ts_acc, stat = valid(model, device, test_iter)\n",
        "    print('Valid Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, ts_loss, ts_acc))\n",
        "    \n",
        "    if ts_acc > best_test_acc:\n",
        "        best_test_acc = ts_acc\n",
        "        print(stat)\n",
        "        #save paras(snapshot)\n",
        "        print(\"model saves at {} accuracy\".format(best_test_acc))\n",
        "        torch.save(model.state_dict(), \"textCNN_supervise_best\")\n",
        "        \n",
        "    train_loss.append(tr_loss)\n",
        "    train_acc.append(tr_acc)\n",
        "    test_loss.append(ts_loss)\n",
        "    test_acc.append(ts_acc)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 \t Loss: 0.0103516497194767 \t Accuracy: 64.01000213623047%\n",
            "Valid Epoch: 1 \t Loss: 0.010906264046827952 \t Accuracy: 54.46666717529297%\n",
            "{'fake_recall': 0.539795918367347, 'fake_precision': 0.991875, 'fake_f1': 0.6991189427312775, 'real_recall': 0.7833333333333333, 'real_precision': 0.03357142857142857, 'real_f1': 0.06438356164383562}\n",
            "model saves at 54.46666717529297 accuracy\n",
            "Train Epoch: 2 \t Loss: 0.009653223114212354 \t Accuracy: 67.09333038330078%\n",
            "Valid Epoch: 2 \t Loss: 0.010121845960617066 \t Accuracy: 62.666664123535156%\n",
            "{'fake_recall': 0.5909090909090909, 'fake_precision': 0.975, 'fake_f1': 0.7358490566037736, 'real_recall': 0.8888888888888888, 'real_precision': 0.22857142857142856, 'real_f1': 0.3636363636363636}\n",
            "model saves at 62.666664123535156 accuracy\n",
            "Train Epoch: 3 \t Loss: 0.009187939013044039 \t Accuracy: 69.49666595458984%\n",
            "Valid Epoch: 3 \t Loss: 0.009624328861633936 \t Accuracy: 66.83333587646484%\n",
            "{'fake_recall': 0.6370638876302673, 'fake_precision': 0.87875, 'fake_f1': 0.7386393485684266, 'real_recall': 0.755359394703657, 'real_precision': 0.4278571428571429, 'real_f1': 0.5462836297309621}\n",
            "model saves at 66.83333587646484 accuracy\n",
            "Train Epoch: 4 \t Loss: 0.008634183410803476 \t Accuracy: 72.3499984741211%\n",
            "Valid Epoch: 4 \t Loss: 0.01003746896982193 \t Accuracy: 65.33333587646484%\n",
            "Train Epoch: 5 \t Loss: 0.007956489982207616 \t Accuracy: 75.11333465576172%\n",
            "Valid Epoch: 5 \t Loss: 0.009526479681332906 \t Accuracy: 68.26666259765625%\n",
            "{'fake_recall': 0.6498612395929695, 'fake_precision': 0.878125, 'fake_f1': 0.746943115364168, 'real_recall': 0.7673031026252983, 'real_precision': 0.4592857142857143, 'real_f1': 0.5746201966041108}\n",
            "model saves at 68.26666259765625 accuracy\n",
            "Train Epoch: 6 \t Loss: 0.00729777167836825 \t Accuracy: 77.92333221435547%\n",
            "Valid Epoch: 6 \t Loss: 0.010049073169628778 \t Accuracy: 67.9000015258789%\n",
            "Train Epoch: 7 \t Loss: 0.006654235510031382 \t Accuracy: 80.43000030517578%\n",
            "Valid Epoch: 7 \t Loss: 0.010528511643409729 \t Accuracy: 66.46666717529297%\n",
            "Train Epoch: 8 \t Loss: 0.006204890534778436 \t Accuracy: 81.91000366210938%\n",
            "Valid Epoch: 8 \t Loss: 0.010941548198461533 \t Accuracy: 66.5999984741211%\n",
            "Train Epoch: 9 \t Loss: 0.0057521613776683805 \t Accuracy: 83.43000030517578%\n",
            "Valid Epoch: 9 \t Loss: 0.011539759029944738 \t Accuracy: 66.16666412353516%\n",
            "Train Epoch: 10 \t Loss: 0.005496142275134722 \t Accuracy: 84.01000213623047%\n",
            "Valid Epoch: 10 \t Loss: 0.012318579296271006 \t Accuracy: 65.80000305175781%\n",
            "Train Epoch: 11 \t Loss: 0.005238639385501544 \t Accuracy: 84.91999816894531%\n",
            "Valid Epoch: 11 \t Loss: 0.013088288605213166 \t Accuracy: 65.93333435058594%\n",
            "Train Epoch: 12 \t Loss: 0.0050478299568096794 \t Accuracy: 85.39666748046875%\n",
            "Valid Epoch: 12 \t Loss: 0.01331206750869751 \t Accuracy: 65.43333435058594%\n",
            "Train Epoch: 13 \t Loss: 0.00485664194871982 \t Accuracy: 85.88666534423828%\n",
            "Valid Epoch: 13 \t Loss: 0.012942052155733108 \t Accuracy: 66.69999694824219%\n",
            "Train Epoch: 14 \t Loss: 0.004753542207678159 \t Accuracy: 86.13999938964844%\n",
            "Valid Epoch: 14 \t Loss: 0.014368467350800832 \t Accuracy: 66.4000015258789%\n",
            "Train Epoch: 15 \t Loss: 0.004651264746983846 \t Accuracy: 86.51000213623047%\n",
            "Valid Epoch: 15 \t Loss: 0.013805790652831395 \t Accuracy: 66.73332977294922%\n",
            "Train Epoch: 16 \t Loss: 0.004538757140437762 \t Accuracy: 86.5433349609375%\n",
            "Valid Epoch: 16 \t Loss: 0.014264011204242706 \t Accuracy: 66.0%\n",
            "Train Epoch: 17 \t Loss: 0.004483294174571832 \t Accuracy: 86.76667022705078%\n",
            "Valid Epoch: 17 \t Loss: 0.01517736252148946 \t Accuracy: 66.43333435058594%\n",
            "Train Epoch: 18 \t Loss: 0.0043520951094726725 \t Accuracy: 86.97000122070312%\n",
            "Valid Epoch: 18 \t Loss: 0.01484210326274236 \t Accuracy: 65.4000015258789%\n",
            "Train Epoch: 19 \t Loss: 0.004312441305319468 \t Accuracy: 87.07333374023438%\n",
            "Valid Epoch: 19 \t Loss: 0.014693539559841156 \t Accuracy: 66.0%\n",
            "Train Epoch: 20 \t Loss: 0.0042573655687272545 \t Accuracy: 87.48999786376953%\n",
            "Valid Epoch: 20 \t Loss: 0.014753451724847157 \t Accuracy: 67.53333282470703%\n",
            "Train Epoch: 21 \t Loss: 0.004227314070115487 \t Accuracy: 87.17333221435547%\n",
            "Valid Epoch: 21 \t Loss: 0.015909133851528166 \t Accuracy: 65.9000015258789%\n",
            "Train Epoch: 22 \t Loss: 0.004174175141255061 \t Accuracy: 87.27333068847656%\n",
            "Valid Epoch: 22 \t Loss: 0.015791610181331635 \t Accuracy: 65.9000015258789%\n",
            "Train Epoch: 23 \t Loss: 0.004117840059598287 \t Accuracy: 87.37999725341797%\n",
            "Valid Epoch: 23 \t Loss: 0.0158540674050649 \t Accuracy: 65.16666412353516%\n",
            "Train Epoch: 24 \t Loss: 0.004117729321122169 \t Accuracy: 87.3566665649414%\n",
            "Valid Epoch: 24 \t Loss: 0.01648973230520884 \t Accuracy: 64.46666717529297%\n",
            "Train Epoch: 25 \t Loss: 0.004060595040520032 \t Accuracy: 87.63333129882812%\n",
            "Valid Epoch: 25 \t Loss: 0.015415918370087942 \t Accuracy: 66.0999984741211%\n",
            "Train Epoch: 26 \t Loss: 0.003984832515070836 \t Accuracy: 87.7066650390625%\n",
            "Valid Epoch: 26 \t Loss: 0.017721602916717528 \t Accuracy: 66.63333129882812%\n",
            "Train Epoch: 27 \t Loss: 0.004008106407523155 \t Accuracy: 87.50333404541016%\n",
            "Valid Epoch: 27 \t Loss: 0.017671634217103323 \t Accuracy: 64.03333282470703%\n",
            "Train Epoch: 28 \t Loss: 0.003990350169688463 \t Accuracy: 87.68000030517578%\n",
            "Valid Epoch: 28 \t Loss: 0.017133690774440765 \t Accuracy: 67.03333282470703%\n",
            "Train Epoch: 29 \t Loss: 0.003970873170346022 \t Accuracy: 87.51000213623047%\n",
            "Valid Epoch: 29 \t Loss: 0.016558867514133453 \t Accuracy: 65.83333587646484%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj93cXCRDtN2"
      },
      "source": [
        "# Automatically annotated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEdZY8XKVnA9"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "import math"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAA73dcmYti0"
      },
      "source": [
        "# compute cosine similarity matrix\n",
        "feature = logit_vec.transform(unlabeled_lr[\"Title\"][:30000])"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Gg0ADy_aYAr",
        "outputId": "05f1b226-ac9a-4381-ea6d-af51168e2c1d"
      },
      "source": [
        "cos_sim = cosine_similarity(feature)\n",
        "cos_sim.shape"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 30000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KvJ1t5qc-t3",
        "outputId": "66be3acc-a104-4d98-f58b-589a0581f786"
      },
      "source": [
        "# detector\n",
        "train_cut_label = train_cut[\"label\"]\n",
        "train_cut_corpus = train_cut[\"Title\"]\n",
        "y_test = test_cut.label\n",
        "X_test = test_cut[\"Title\"]\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_cut_corpus, train_cut_label, test_size=0.2, random_state=42)\n",
        "\n",
        "vec = TfidfVectorizer(tokenizer=word_cut)\n",
        "X_train_tfidf = vec.fit_transform(X_train)\n",
        "X_val_tfidf = vec.transform(X_val)\n",
        "X_test_tfidf = vec.transform(X_test)\n",
        "\n",
        "lr_automatic = LogisticRegression()\n",
        "lr_automatic.fit(X_train_tfidf, y_train)\n",
        "print(classification_report(y_val, lr_automatic.predict(X_val_tfidf)))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.94      0.93       422\n",
            "           1       0.93      0.92      0.92       378\n",
            "\n",
            "    accuracy                           0.93       800\n",
            "   macro avg       0.93      0.93      0.93       800\n",
            "weighted avg       0.93      0.93      0.93       800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyBkYKT-Dzy3"
      },
      "source": [
        "# define reinforcement learning pipeline\n",
        "def action(w1, state):\n",
        "    # calculate prob given state and weight\n",
        "    prob = sigmoid(np.dot(w1, state))\n",
        "    # randomly choose action from gaussian distribution\n",
        "    x = random.uniform(0,1)\n",
        "    act = 0\n",
        "    if x <= prob:\n",
        "        act = 1\n",
        "    return act, prob\n",
        "def get_state(index, row, chosen, prev_state):\n",
        "    feature = vec.transform([row[\"Title\"]])\n",
        "    prob = lr_automatic.predict_proba(feature)\n",
        "\n",
        "    # get log prob from annotator and detector\n",
        "    prob_annotator = row[\"lr prob\"]\n",
        "    prob_detector = np.amax(prob, axis = 1)[0]\n",
        "\n",
        "    # cosine similarity between chosen samples and current one\n",
        "    cos = max(cos_sim[index][chosen])\n",
        "\n",
        "    # weak label\n",
        "    label = row[\"lr label\"]\n",
        "\n",
        "    # combined\n",
        "    temp = [prob_annotator, prob_detector, cos, label]\n",
        "\n",
        "    # taking average on the previously chosen states\n",
        "    avg = np.mean(prev_state, axis=0)\n",
        "    \n",
        "    # return concat state\n",
        "    return temp, np.concatenate((temp, avg), axis = None)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + math.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK78OFIb0TKx",
        "outputId": "c752c332-cec3-4bf0-efd3-95cfc88b3739"
      },
      "source": [
        "# main loop of reinforcement learning\n",
        "# stop until\n",
        "alpha = 0.001\n",
        "count = 0\n",
        "bag_size = 100\n",
        "bag_count = 0\n",
        "teach = 0\n",
        "chosen = [0]\n",
        "p = []\n",
        "act_total = []\n",
        "state_total = []\n",
        "prev_state = [[0,0,0,0]]\n",
        "w1 = np.random.randn(8)\n",
        "best_acc = 0\n",
        "w_best = None\n",
        "acc = accuracy_score(y_test, lr_automatic.predict(X_test_tfidf))\n",
        "for index, row in unlabeled_lr[:30000].iterrows():\n",
        "    # store the values of prob and action for gradients\n",
        "    cur_state, all_state = get_state(index, row, chosen, prev_state)\n",
        "    act, prob = action(w1, all_state)\n",
        "        \n",
        "    # record state informatin if chosen\n",
        "    if act == 1:\n",
        "        chosen.append(index)\n",
        "        prev_state.append(cur_state)\n",
        "        p.append(prob)\n",
        "    else:\n",
        "        p.append(1 - prob)\n",
        "    act_total.append(act)\n",
        "    state_total.append(all_state)\n",
        "\n",
        "    # update weights & switch until bag size reached\n",
        "    if bag_count >= bag_size:\n",
        "        # compute policy gradient with the gradient of log sigmoid\n",
        "        # (a - p) * x\n",
        "        sub = np.array([act_total]) - np.array([p])\n",
        "        gradient = np.dot(sub, state_total)\n",
        "\n",
        "        # compute R with acc and acc_k\n",
        "        selected = unlabeled_lr.iloc[chosen]\n",
        "        selected_label = selected[\"lr label\"]\n",
        "        selected_corpus = selected[\"Title\"]\n",
        "        train_label = pd.concat([y_train, selected_label], ignore_index=True)\n",
        "        train_corpus = pd.concat([X_train, selected_corpus], ignore_index=True)\n",
        "        X_train_tfidf = vec.transform(train_corpus)\n",
        "\n",
        "        # retrain the LR\n",
        "        lr_temp = LogisticRegression().fit(X_train_tfidf, train_label)\n",
        "        acc_k = accuracy_score(y_test, lr_temp.predict(X_test_tfidf))\n",
        "        print(\"Currently at step \" + str(count) + \": \")\n",
        "        print(acc_k)\n",
        "\n",
        "\n",
        "        # update weights\n",
        "        w1 = w1 + alpha * gradient * (acc_k - acc)\n",
        "        if acc_k > best_acc:\n",
        "            w_best = w1\n",
        "        # update parameters\n",
        "        teach += 1\n",
        "        bag_count = 0\n",
        "        chosen = [0]\n",
        "        prev_state = [[0,0,0,0]]\n",
        "        p = []\n",
        "        act_total = []\n",
        "        state_total = []\n",
        "        continue\n",
        "    bag_count += 1\n",
        "    count += 1"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently at step 100: \n",
            "0.7406666666666667\n",
            "Currently at step 200: \n",
            "0.7373333333333333\n",
            "Currently at step 300: \n",
            "0.751\n",
            "Currently at step 400: \n",
            "0.7346666666666667\n",
            "Currently at step 500: \n",
            "0.733\n",
            "Currently at step 600: \n",
            "0.7436666666666667\n",
            "Currently at step 700: \n",
            "0.7356666666666667\n",
            "Currently at step 800: \n",
            "0.7416666666666667\n",
            "Currently at step 900: \n",
            "0.7363333333333333\n",
            "Currently at step 1000: \n",
            "0.7356666666666667\n",
            "Currently at step 1100: \n",
            "0.7463333333333333\n",
            "Currently at step 1200: \n",
            "0.744\n",
            "Currently at step 1300: \n",
            "0.7413333333333333\n",
            "Currently at step 1400: \n",
            "0.7456666666666667\n",
            "Currently at step 1500: \n",
            "0.7406666666666667\n",
            "Currently at step 1600: \n",
            "0.739\n",
            "Currently at step 1700: \n",
            "0.742\n",
            "Currently at step 1800: \n",
            "0.7416666666666667\n",
            "Currently at step 1900: \n",
            "0.7336666666666667\n",
            "Currently at step 2000: \n",
            "0.7423333333333333\n",
            "Currently at step 2100: \n",
            "0.7556666666666667\n",
            "Currently at step 2200: \n",
            "0.7386666666666667\n",
            "Currently at step 2300: \n",
            "0.7363333333333333\n",
            "Currently at step 2400: \n",
            "0.742\n",
            "Currently at step 2500: \n",
            "0.7493333333333333\n",
            "Currently at step 2600: \n",
            "0.7456666666666667\n",
            "Currently at step 2700: \n",
            "0.734\n",
            "Currently at step 2800: \n",
            "0.7416666666666667\n",
            "Currently at step 2900: \n",
            "0.744\n",
            "Currently at step 3000: \n",
            "0.7406666666666667\n",
            "Currently at step 3100: \n",
            "0.7323333333333333\n",
            "Currently at step 3200: \n",
            "0.7393333333333333\n",
            "Currently at step 3300: \n",
            "0.7526666666666667\n",
            "Currently at step 3400: \n",
            "0.742\n",
            "Currently at step 3500: \n",
            "0.7386666666666667\n",
            "Currently at step 3600: \n",
            "0.739\n",
            "Currently at step 3700: \n",
            "0.7366666666666667\n",
            "Currently at step 3800: \n",
            "0.7436666666666667\n",
            "Currently at step 3900: \n",
            "0.737\n",
            "Currently at step 4000: \n",
            "0.7433333333333333\n",
            "Currently at step 4100: \n",
            "0.737\n",
            "Currently at step 4200: \n",
            "0.737\n",
            "Currently at step 4300: \n",
            "0.753\n",
            "Currently at step 4400: \n",
            "0.749\n",
            "Currently at step 4500: \n",
            "0.7383333333333333\n",
            "Currently at step 4600: \n",
            "0.7446666666666667\n",
            "Currently at step 4700: \n",
            "0.743\n",
            "Currently at step 4800: \n",
            "0.755\n",
            "Currently at step 4900: \n",
            "0.7423333333333333\n",
            "Currently at step 5000: \n",
            "0.7526666666666667\n",
            "Currently at step 5100: \n",
            "0.7413333333333333\n",
            "Currently at step 5200: \n",
            "0.7276666666666667\n",
            "Currently at step 5300: \n",
            "0.7453333333333333\n",
            "Currently at step 5400: \n",
            "0.7506666666666667\n",
            "Currently at step 5500: \n",
            "0.7343333333333333\n",
            "Currently at step 5600: \n",
            "0.7366666666666667\n",
            "Currently at step 5700: \n",
            "0.741\n",
            "Currently at step 5800: \n",
            "0.7413333333333333\n",
            "Currently at step 5900: \n",
            "0.741\n",
            "Currently at step 6000: \n",
            "0.74\n",
            "Currently at step 6100: \n",
            "0.7406666666666667\n",
            "Currently at step 6200: \n",
            "0.747\n",
            "Currently at step 6300: \n",
            "0.7536666666666667\n",
            "Currently at step 6400: \n",
            "0.7376666666666667\n",
            "Currently at step 6500: \n",
            "0.7423333333333333\n",
            "Currently at step 6600: \n",
            "0.7456666666666667\n",
            "Currently at step 6700: \n",
            "0.733\n",
            "Currently at step 6800: \n",
            "0.7496666666666667\n",
            "Currently at step 6900: \n",
            "0.7413333333333333\n",
            "Currently at step 7000: \n",
            "0.729\n",
            "Currently at step 7100: \n",
            "0.7453333333333333\n",
            "Currently at step 7200: \n",
            "0.737\n",
            "Currently at step 7300: \n",
            "0.7453333333333333\n",
            "Currently at step 7400: \n",
            "0.7486666666666667\n",
            "Currently at step 7500: \n",
            "0.758\n",
            "Currently at step 7600: \n",
            "0.74\n",
            "Currently at step 7700: \n",
            "0.753\n",
            "Currently at step 7800: \n",
            "0.7466666666666667\n",
            "Currently at step 7900: \n",
            "0.7416666666666667\n",
            "Currently at step 8000: \n",
            "0.741\n",
            "Currently at step 8100: \n",
            "0.7446666666666667\n",
            "Currently at step 8200: \n",
            "0.745\n",
            "Currently at step 8300: \n",
            "0.7446666666666667\n",
            "Currently at step 8400: \n",
            "0.742\n",
            "Currently at step 8500: \n",
            "0.7443333333333333\n",
            "Currently at step 8600: \n",
            "0.7413333333333333\n",
            "Currently at step 8700: \n",
            "0.7426666666666667\n",
            "Currently at step 8800: \n",
            "0.742\n",
            "Currently at step 8900: \n",
            "0.7393333333333333\n",
            "Currently at step 9000: \n",
            "0.7513333333333333\n",
            "Currently at step 9100: \n",
            "0.745\n",
            "Currently at step 9200: \n",
            "0.7456666666666667\n",
            "Currently at step 9300: \n",
            "0.7536666666666667\n",
            "Currently at step 9400: \n",
            "0.7463333333333333\n",
            "Currently at step 9500: \n",
            "0.7303333333333333\n",
            "Currently at step 9600: \n",
            "0.7493333333333333\n",
            "Currently at step 9700: \n",
            "0.736\n",
            "Currently at step 9800: \n",
            "0.7436666666666667\n",
            "Currently at step 9900: \n",
            "0.7446666666666667\n",
            "Currently at step 10000: \n",
            "0.746\n",
            "Currently at step 10100: \n",
            "0.7373333333333333\n",
            "Currently at step 10200: \n",
            "0.7413333333333333\n",
            "Currently at step 10300: \n",
            "0.736\n",
            "Currently at step 10400: \n",
            "0.7396666666666667\n",
            "Currently at step 10500: \n",
            "0.753\n",
            "Currently at step 10600: \n",
            "0.737\n",
            "Currently at step 10700: \n",
            "0.744\n",
            "Currently at step 10800: \n",
            "0.7446666666666667\n",
            "Currently at step 10900: \n",
            "0.7643333333333333\n",
            "Currently at step 11000: \n",
            "0.7473333333333333\n",
            "Currently at step 11100: \n",
            "0.736\n",
            "Currently at step 11200: \n",
            "0.749\n",
            "Currently at step 11300: \n",
            "0.7393333333333333\n",
            "Currently at step 11400: \n",
            "0.7426666666666667\n",
            "Currently at step 11500: \n",
            "0.7513333333333333\n",
            "Currently at step 11600: \n",
            "0.7383333333333333\n",
            "Currently at step 11700: \n",
            "0.7416666666666667\n",
            "Currently at step 11800: \n",
            "0.7353333333333333\n",
            "Currently at step 11900: \n",
            "0.7423333333333333\n",
            "Currently at step 12000: \n",
            "0.7446666666666667\n",
            "Currently at step 12100: \n",
            "0.7396666666666667\n",
            "Currently at step 12200: \n",
            "0.745\n",
            "Currently at step 12300: \n",
            "0.7413333333333333\n",
            "Currently at step 12400: \n",
            "0.7453333333333333\n",
            "Currently at step 12500: \n",
            "0.7393333333333333\n",
            "Currently at step 12600: \n",
            "0.735\n",
            "Currently at step 12700: \n",
            "0.7576666666666667\n",
            "Currently at step 12800: \n",
            "0.75\n",
            "Currently at step 12900: \n",
            "0.7426666666666667\n",
            "Currently at step 13000: \n",
            "0.7413333333333333\n",
            "Currently at step 13100: \n",
            "0.7376666666666667\n",
            "Currently at step 13200: \n",
            "0.7393333333333333\n",
            "Currently at step 13300: \n",
            "0.7416666666666667\n",
            "Currently at step 13400: \n",
            "0.7393333333333333\n",
            "Currently at step 13500: \n",
            "0.75\n",
            "Currently at step 13600: \n",
            "0.732\n",
            "Currently at step 13700: \n",
            "0.7423333333333333\n",
            "Currently at step 13800: \n",
            "0.7406666666666667\n",
            "Currently at step 13900: \n",
            "0.746\n",
            "Currently at step 14000: \n",
            "0.7503333333333333\n",
            "Currently at step 14100: \n",
            "0.7413333333333333\n",
            "Currently at step 14200: \n",
            "0.7356666666666667\n",
            "Currently at step 14300: \n",
            "0.7456666666666667\n",
            "Currently at step 14400: \n",
            "0.737\n",
            "Currently at step 14500: \n",
            "0.733\n",
            "Currently at step 14600: \n",
            "0.7426666666666667\n",
            "Currently at step 14700: \n",
            "0.743\n",
            "Currently at step 14800: \n",
            "0.7363333333333333\n",
            "Currently at step 14900: \n",
            "0.747\n",
            "Currently at step 15000: \n",
            "0.7406666666666667\n",
            "Currently at step 15100: \n",
            "0.7396666666666667\n",
            "Currently at step 15200: \n",
            "0.7376666666666667\n",
            "Currently at step 15300: \n",
            "0.741\n",
            "Currently at step 15400: \n",
            "0.742\n",
            "Currently at step 15500: \n",
            "0.745\n",
            "Currently at step 15600: \n",
            "0.7413333333333333\n",
            "Currently at step 15700: \n",
            "0.7443333333333333\n",
            "Currently at step 15800: \n",
            "0.7453333333333333\n",
            "Currently at step 15900: \n",
            "0.7343333333333333\n",
            "Currently at step 16000: \n",
            "0.743\n",
            "Currently at step 16100: \n",
            "0.7356666666666667\n",
            "Currently at step 16200: \n",
            "0.736\n",
            "Currently at step 16300: \n",
            "0.7453333333333333\n",
            "Currently at step 16400: \n",
            "0.7453333333333333\n",
            "Currently at step 16500: \n",
            "0.742\n",
            "Currently at step 16600: \n",
            "0.7506666666666667\n",
            "Currently at step 16700: \n",
            "0.7423333333333333\n",
            "Currently at step 16800: \n",
            "0.736\n",
            "Currently at step 16900: \n",
            "0.7436666666666667\n",
            "Currently at step 17000: \n",
            "0.74\n",
            "Currently at step 17100: \n",
            "0.741\n",
            "Currently at step 17200: \n",
            "0.7353333333333333\n",
            "Currently at step 17300: \n",
            "0.7296666666666667\n",
            "Currently at step 17400: \n",
            "0.748\n",
            "Currently at step 17500: \n",
            "0.7426666666666667\n",
            "Currently at step 17600: \n",
            "0.7483333333333333\n",
            "Currently at step 17700: \n",
            "0.739\n",
            "Currently at step 17800: \n",
            "0.7503333333333333\n",
            "Currently at step 17900: \n",
            "0.7473333333333333\n",
            "Currently at step 18000: \n",
            "0.7536666666666667\n",
            "Currently at step 18100: \n",
            "0.7513333333333333\n",
            "Currently at step 18200: \n",
            "0.738\n",
            "Currently at step 18300: \n",
            "0.743\n",
            "Currently at step 18400: \n",
            "0.735\n",
            "Currently at step 18500: \n",
            "0.737\n",
            "Currently at step 18600: \n",
            "0.734\n",
            "Currently at step 18700: \n",
            "0.7676666666666667\n",
            "Currently at step 18800: \n",
            "0.7376666666666667\n",
            "Currently at step 18900: \n",
            "0.7366666666666667\n",
            "Currently at step 19000: \n",
            "0.742\n",
            "Currently at step 19100: \n",
            "0.7476666666666667\n",
            "Currently at step 19200: \n",
            "0.7323333333333333\n",
            "Currently at step 19300: \n",
            "0.747\n",
            "Currently at step 19400: \n",
            "0.7496666666666667\n",
            "Currently at step 19500: \n",
            "0.7393333333333333\n",
            "Currently at step 19600: \n",
            "0.7406666666666667\n",
            "Currently at step 19700: \n",
            "0.7373333333333333\n",
            "Currently at step 19800: \n",
            "0.7423333333333333\n",
            "Currently at step 19900: \n",
            "0.742\n",
            "Currently at step 20000: \n",
            "0.744\n",
            "Currently at step 20100: \n",
            "0.748\n",
            "Currently at step 20200: \n",
            "0.7416666666666667\n",
            "Currently at step 20300: \n",
            "0.732\n",
            "Currently at step 20400: \n",
            "0.7486666666666667\n",
            "Currently at step 20500: \n",
            "0.739\n",
            "Currently at step 20600: \n",
            "0.743\n",
            "Currently at step 20700: \n",
            "0.7383333333333333\n",
            "Currently at step 20800: \n",
            "0.7396666666666667\n",
            "Currently at step 20900: \n",
            "0.745\n",
            "Currently at step 21000: \n",
            "0.7456666666666667\n",
            "Currently at step 21100: \n",
            "0.739\n",
            "Currently at step 21200: \n",
            "0.7376666666666667\n",
            "Currently at step 21300: \n",
            "0.7473333333333333\n",
            "Currently at step 21400: \n",
            "0.7416666666666667\n",
            "Currently at step 21500: \n",
            "0.7383333333333333\n",
            "Currently at step 21600: \n",
            "0.747\n",
            "Currently at step 21700: \n",
            "0.7426666666666667\n",
            "Currently at step 21800: \n",
            "0.746\n",
            "Currently at step 21900: \n",
            "0.7433333333333333\n",
            "Currently at step 22000: \n",
            "0.7436666666666667\n",
            "Currently at step 22100: \n",
            "0.7443333333333333\n",
            "Currently at step 22200: \n",
            "0.741\n",
            "Currently at step 22300: \n",
            "0.7346666666666667\n",
            "Currently at step 22400: \n",
            "0.7413333333333333\n",
            "Currently at step 22500: \n",
            "0.7423333333333333\n",
            "Currently at step 22600: \n",
            "0.7416666666666667\n",
            "Currently at step 22700: \n",
            "0.7443333333333333\n",
            "Currently at step 22800: \n",
            "0.7386666666666667\n",
            "Currently at step 22900: \n",
            "0.736\n",
            "Currently at step 23000: \n",
            "0.7383333333333333\n",
            "Currently at step 23100: \n",
            "0.737\n",
            "Currently at step 23200: \n",
            "0.7543333333333333\n",
            "Currently at step 23300: \n",
            "0.746\n",
            "Currently at step 23400: \n",
            "0.741\n",
            "Currently at step 23500: \n",
            "0.7513333333333333\n",
            "Currently at step 23600: \n",
            "0.744\n",
            "Currently at step 23700: \n",
            "0.7463333333333333\n",
            "Currently at step 23800: \n",
            "0.7443333333333333\n",
            "Currently at step 23900: \n",
            "0.7426666666666667\n",
            "Currently at step 24000: \n",
            "0.745\n",
            "Currently at step 24100: \n",
            "0.7343333333333333\n",
            "Currently at step 24200: \n",
            "0.7383333333333333\n",
            "Currently at step 24300: \n",
            "0.7523333333333333\n",
            "Currently at step 24400: \n",
            "0.741\n",
            "Currently at step 24500: \n",
            "0.7363333333333333\n",
            "Currently at step 24600: \n",
            "0.736\n",
            "Currently at step 24700: \n",
            "0.7393333333333333\n",
            "Currently at step 24800: \n",
            "0.7426666666666667\n",
            "Currently at step 24900: \n",
            "0.7466666666666667\n",
            "Currently at step 25000: \n",
            "0.7346666666666667\n",
            "Currently at step 25100: \n",
            "0.7533333333333333\n",
            "Currently at step 25200: \n",
            "0.744\n",
            "Currently at step 25300: \n",
            "0.7403333333333333\n",
            "Currently at step 25400: \n",
            "0.7503333333333333\n",
            "Currently at step 25500: \n",
            "0.7383333333333333\n",
            "Currently at step 25600: \n",
            "0.7356666666666667\n",
            "Currently at step 25700: \n",
            "0.7366666666666667\n",
            "Currently at step 25800: \n",
            "0.742\n",
            "Currently at step 25900: \n",
            "0.742\n",
            "Currently at step 26000: \n",
            "0.7363333333333333\n",
            "Currently at step 26100: \n",
            "0.7433333333333333\n",
            "Currently at step 26200: \n",
            "0.7413333333333333\n",
            "Currently at step 26300: \n",
            "0.741\n",
            "Currently at step 26400: \n",
            "0.7416666666666667\n",
            "Currently at step 26500: \n",
            "0.739\n",
            "Currently at step 26600: \n",
            "0.737\n",
            "Currently at step 26700: \n",
            "0.75\n",
            "Currently at step 26800: \n",
            "0.7396666666666667\n",
            "Currently at step 26900: \n",
            "0.75\n",
            "Currently at step 27000: \n",
            "0.742\n",
            "Currently at step 27100: \n",
            "0.7436666666666667\n",
            "Currently at step 27200: \n",
            "0.7373333333333333\n",
            "Currently at step 27300: \n",
            "0.7433333333333333\n",
            "Currently at step 27400: \n",
            "0.7493333333333333\n",
            "Currently at step 27500: \n",
            "0.741\n",
            "Currently at step 27600: \n",
            "0.7443333333333333\n",
            "Currently at step 27700: \n",
            "0.7383333333333333\n",
            "Currently at step 27800: \n",
            "0.7343333333333333\n",
            "Currently at step 27900: \n",
            "0.7373333333333333\n",
            "Currently at step 28000: \n",
            "0.7426666666666667\n",
            "Currently at step 28100: \n",
            "0.739\n",
            "Currently at step 28200: \n",
            "0.734\n",
            "Currently at step 28300: \n",
            "0.7436666666666667\n",
            "Currently at step 28400: \n",
            "0.7416666666666667\n",
            "Currently at step 28500: \n",
            "0.739\n",
            "Currently at step 28600: \n",
            "0.7416666666666667\n",
            "Currently at step 28700: \n",
            "0.749\n",
            "Currently at step 28800: \n",
            "0.7356666666666667\n",
            "Currently at step 28900: \n",
            "0.7386666666666667\n",
            "Currently at step 29000: \n",
            "0.7383333333333333\n",
            "Currently at step 29100: \n",
            "0.745\n",
            "Currently at step 29200: \n",
            "0.7353333333333333\n",
            "Currently at step 29300: \n",
            "0.741\n",
            "Currently at step 29400: \n",
            "0.732\n",
            "Currently at step 29500: \n",
            "0.7443333333333333\n",
            "Currently at step 29600: \n",
            "0.744\n",
            "Currently at step 29700: \n",
            "0.748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4gCJOwC0Tb4"
      },
      "source": [
        "# Using trained policy network to select samples\n",
        "chosen = [0]\n",
        "total_chosen = []\n",
        "prev_state = [[0,0,0,0]]\n",
        "acc = accuracy_score(y_test, lr_automatic.predict(X_test_tfidf))\n",
        "for index, row in unlabeled_lr[:30000].iterrows():\n",
        "    # store the values of prob and action for gradients\n",
        "    cur_state, all_state = get_state(index, row, chosen, prev_state)\n",
        "    act, prob = action(w_best, all_state)\n",
        "        \n",
        "    # record state informatin if chosen\n",
        "    if act == 1:\n",
        "        chosen.append(index)\n",
        "        total_chosen.append(index)\n",
        "        prev_state.append(cur_state)\n",
        "\n",
        "    # update weights & switch until bag size reached\n",
        "    if bag_count >= bag_size:\n",
        "        # update parameters\n",
        "        bag_count = 0\n",
        "        chosen = [0]\n",
        "        prev_state = [[0,0,0,0]]\n",
        "        continue\n",
        "    bag_count += 1\n",
        "    count += 1\n",
        "    # teacher force"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZAt4hktj58m",
        "outputId": "10f92570-3882-4b69-e05e-09726adade3e"
      },
      "source": [
        "len(total_chosen)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24495"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDR9q_0NH948"
      },
      "source": [
        "# Without training samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ1_ss_kIBWX",
        "outputId": "20646611-60cc-4bf4-b43c-23eb5cac9102"
      },
      "source": [
        "# get rid of first place holder\n",
        "selected = unlabeled_lr.iloc[total_chosen[1:]]\n",
        "selected_label = selected[\"lr label\"]\n",
        "selected_corpus = selected[\"Title\"]\n",
        "# train_label = pd.concat([y_train, selected_label], ignore_index=True)\n",
        "# train_corpus = pd.concat([X_train, selected_corpus], ignore_index=True)\n",
        "train_label = selected_label\n",
        "train_corpus = selected_corpus\n",
        "final_vec = TfidfVectorizer(tokenizer=word_cut)\n",
        "X_train_tfidf = final_vec.fit_transform(train_corpus)\n",
        "X_test_tfidf = final_vec.transform(X_test)\n",
        "\n",
        "# retrain the LR\n",
        "lr_final = LogisticRegression().fit(X_train_tfidf, train_label)\n",
        "print(classification_report(y_test, lr_final.predict(X_test_tfidf), digits = 4))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6451    0.9281    0.7611      1600\n",
            "           1     0.8352    0.4164    0.5558      1400\n",
            "\n",
            "    accuracy                         0.6893      3000\n",
            "   macro avg     0.7402    0.6723    0.6585      3000\n",
            "weighted avg     0.7338    0.6893    0.6653      3000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVxaqebNz_v-",
        "outputId": "66dbf538-7008-4a01-ca7c-9de36ab095e6"
      },
      "source": [
        "svc = LinearSVC()\n",
        "svc.fit(X_train_tfidf, train_label)\n",
        "print(classification_report(y_test, svc.predict(X_test_tfidf), digits=4))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6639    0.8000    0.7256      1600\n",
            "           1     0.7015    0.5371    0.6084      1400\n",
            "\n",
            "    accuracy                         0.6773      3000\n",
            "   macro avg     0.6827    0.6686    0.6670      3000\n",
            "weighted avg     0.6814    0.6773    0.6709      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u85HJqcE0HXK",
        "outputId": "cd220dfb-6f10-4174-88b8-12e791763a75"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train_tfidf, train_label)\n",
        "print(classification_report(y_test, rf.predict(X_test_tfidf), digits=4))"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6340    0.8781    0.7364      1600\n",
            "           1     0.7513    0.4207    0.5394      1400\n",
            "\n",
            "    accuracy                         0.6647      3000\n",
            "   macro avg     0.6927    0.6494    0.6379      3000\n",
            "weighted avg     0.6887    0.6647    0.6444      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERvlTPY6H5Ap"
      },
      "source": [
        "## With training samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjVbt7RNPXQl",
        "outputId": "bf23e7b7-48f9-495b-f582-bc5995912d02"
      },
      "source": [
        "selected = unlabeled_lr.iloc[total_chosen[1:]]\n",
        "selected_label = selected[\"lr label\"]\n",
        "selected_corpus = selected[\"Title\"]\n",
        "train_label = pd.concat([y_train, selected_label], ignore_index=True)\n",
        "train_corpus = pd.concat([X_train, selected_corpus], ignore_index=True)\n",
        "final_vec = TfidfVectorizer(tokenizer=word_cut)\n",
        "X_train_tfidf = final_vec.fit_transform(train_corpus)\n",
        "X_test_tfidf = final_vec.transform(X_test)\n",
        "\n",
        "# retrain the LR\n",
        "lr_final = LogisticRegression().fit(X_train_tfidf, train_label)\n",
        "print(classification_report(y_test, lr_final.predict(X_test_tfidf), digits = 4))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7242    0.9175    0.8095      1600\n",
            "           1     0.8643    0.6007    0.7088      1400\n",
            "\n",
            "    accuracy                         0.7697      3000\n",
            "   macro avg     0.7943    0.7591    0.7591      3000\n",
            "weighted avg     0.7896    0.7697    0.7625      3000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhP6lRFPPXWk",
        "outputId": "56567d58-e354-436c-f6fc-91e9dcf94499"
      },
      "source": [
        "svc = LinearSVC()\n",
        "svc.fit(X_train_tfidf, train_label)\n",
        "print(classification_report(y_test, svc.predict(X_test_tfidf), digits=4))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7510    0.8163    0.7823      1600\n",
            "           1     0.7669    0.6907    0.7268      1400\n",
            "\n",
            "    accuracy                         0.7577      3000\n",
            "   macro avg     0.7589    0.7535    0.7545      3000\n",
            "weighted avg     0.7584    0.7577    0.7564      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a_TOk2T0OoT",
        "outputId": "e5af2fa6-640b-4c51-882f-96bfb46f8ac7"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train_tfidf, train_label)\n",
        "print(classification_report(y_test, rf.predict(X_test_tfidf), digits=4))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7347    0.8844    0.8026      1600\n",
            "           1     0.8277    0.6350    0.7187      1400\n",
            "\n",
            "    accuracy                         0.7680      3000\n",
            "   macro avg     0.7812    0.7597    0.7606      3000\n",
            "weighted avg     0.7781    0.7680    0.7634      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAbgC1c8EQAk"
      },
      "source": [
        "# convert weak supervised training data to csv file\n",
        "train_weak_cnn = pd.DataFrame(columns=[\"Title\", \"label\"])\n",
        "train_weak_cnn[\"Title\"] = train_corpus\n",
        "train_weak_cnn[\"label\"] = train_label\n",
        "train_weak_cnn.to_csv(\"./train_auto.csv\", index = False)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v4l69a-EQFe"
      },
      "source": [
        "# prepare data for nn\n",
        "nn_text = data.Field(sequential=True, tokenize=word_cut)\n",
        "nn_label = data.Field(sequential=False)\n",
        "train_nn_datafield = [('text', nn_text),  ('label', nn_label)]\n",
        "test_nn_datafield = [('text', nn_text),  ('label', nn_label)]\n",
        "train_supervise = data.TabularDataset(path ='./train_auto.csv',  \n",
        "                             format='csv',\n",
        "                             skip_header = True,\n",
        "                             fields = train_nn_datafield)\n",
        "test_supervise = data.TabularDataset(path ='./test_supervise.csv', \n",
        "                       format='csv',\n",
        "                       skip_header = True,\n",
        "                       fields=test_nn_datafield)\n",
        "nn_text.build_vocab(train_supervise)\n",
        "nn_label.build_vocab(train_supervise)\n",
        "nn_vocab = nn_text.vocab\n",
        "\n",
        "# set iterator for batch optimization\n",
        "train_iter = data.Iterator(\n",
        "        train_supervise, \n",
        "        batch_size=64,\n",
        "        device=torch.device('cuda'), \n",
        "        sort_within_batch=False,\n",
        "        repeat=False)\n",
        "\n",
        "test_iter = data.Iterator(test_supervise, batch_size=64, device=torch.device('cuda'), \n",
        "                     sort_within_batch=False, repeat=False)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc_vo4FhEQJp",
        "outputId": "d90dc967-3f41-4dc1-d7bf-aaed636dab38"
      },
      "source": [
        "# train CNN\n",
        "# fine tuning CNN model\n",
        "model = textCNN(nn_vocab, 200, 40, [1,2,3,4,5,6] , 2).to('cuda')\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "best_test_acc = -1\n",
        "\n",
        "# Use GPU if it is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# fine tuning\n",
        "for epoch in range(1, 30):\n",
        "    #train loss\n",
        "    tr_loss, tr_acc = train(model, device, train_iter, optimizer, epoch, 100)\n",
        "    print('Train Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, tr_loss, tr_acc))\n",
        "    \n",
        "    ts_loss, ts_acc, stat = valid(model, device, test_iter)\n",
        "    print('Valid Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, ts_loss, ts_acc))\n",
        "    \n",
        "    if ts_acc > best_test_acc:\n",
        "        best_test_acc = ts_acc\n",
        "        print(stat)\n",
        "        #save paras(snapshot)\n",
        "        print(\"model saves at {} accuracy\".format(best_test_acc))\n",
        "        torch.save(model.state_dict(), \"textCNN_supervise_best\")\n",
        "        \n",
        "    train_loss.append(tr_loss)\n",
        "    train_acc.append(tr_acc)\n",
        "    test_loss.append(ts_loss)\n",
        "    test_acc.append(ts_acc)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 \t Loss: 0.009201257225148208 \t Accuracy: 71.47295379638672%\n",
            "Valid Epoch: 1 \t Loss: 0.009107313921054204 \t Accuracy: 69.83333587646484%\n",
            "{'fake_recall': 0.6483141271873666, 'fake_precision': 0.949375, 'fake_f1': 0.7704793304590414, 'real_recall': 0.8767123287671232, 'real_precision': 0.4114285714285714, 'real_f1': 0.5600388915896937}\n",
            "model saves at 69.83333587646484 accuracy\n",
            "Train Epoch: 2 \t Loss: 0.0069722345948929065 \t Accuracy: 80.7274169921875%\n",
            "Valid Epoch: 2 \t Loss: 0.00834387515981992 \t Accuracy: 74.03333282470703%\n",
            "{'fake_recall': 0.6955693187232015, 'fake_precision': 0.9125, 'fake_f1': 0.7894025412273586, 'real_recall': 0.8446170921198668, 'real_precision': 0.5435714285714286, 'real_f1': 0.661451542807475}\n",
            "model saves at 74.03333282470703 accuracy\n",
            "Train Epoch: 3 \t Loss: 0.005671867929589955 \t Accuracy: 85.21874237060547%\n",
            "Valid Epoch: 3 \t Loss: 0.008752356012662253 \t Accuracy: 73.5999984741211%\n",
            "Train Epoch: 4 \t Loss: 0.004938977327628855 \t Accuracy: 87.08258056640625%\n",
            "Valid Epoch: 4 \t Loss: 0.01049094733595848 \t Accuracy: 71.36666870117188%\n",
            "Train Epoch: 5 \t Loss: 0.004281957223220653 \t Accuracy: 89.11467742919922%\n",
            "Valid Epoch: 5 \t Loss: 0.008128775993982951 \t Accuracy: 76.5999984741211%\n",
            "{'fake_recall': 0.7488913525498891, 'fake_precision': 0.844375, 'fake_f1': 0.7937720329024677, 'real_recall': 0.7918060200668896, 'real_precision': 0.6764285714285714, 'real_f1': 0.7295839753466872}\n",
            "model saves at 76.5999984741211 accuracy\n",
            "Train Epoch: 6 \t Loss: 0.003574431021612233 \t Accuracy: 91.4574203491211%\n",
            "Valid Epoch: 6 \t Loss: 0.009135354926188787 \t Accuracy: 75.66666412353516%\n",
            "Train Epoch: 7 \t Loss: 0.0031635832314413507 \t Accuracy: 92.75174713134766%\n",
            "Valid Epoch: 7 \t Loss: 0.010150674591461817 \t Accuracy: 76.79999542236328%\n",
            "{'fake_recall': 0.7215686274509804, 'fake_precision': 0.92, 'fake_f1': 0.8087912087912088, 'real_recall': 0.8666666666666667, 'real_precision': 0.5942857142857143, 'real_f1': 0.7050847457627119}\n",
            "model saves at 76.79999542236328 accuracy\n",
            "Train Epoch: 8 \t Loss: 0.002852802753348194 \t Accuracy: 93.20476531982422%\n",
            "Valid Epoch: 8 \t Loss: 0.01003025562564532 \t Accuracy: 75.29999542236328%\n",
            "Train Epoch: 9 \t Loss: 0.002742280880311842 \t Accuracy: 94.04608154296875%\n",
            "Valid Epoch: 9 \t Loss: 0.01102675786614418 \t Accuracy: 75.53333282470703%\n",
            "Train Epoch: 10 \t Loss: 0.002584763376826747 \t Accuracy: 94.5767593383789%\n",
            "Valid Epoch: 10 \t Loss: 0.010354636092980703 \t Accuracy: 76.46666717529297%\n",
            "Train Epoch: 11 \t Loss: 0.0024148282926956853 \t Accuracy: 95.0038833618164%\n",
            "Valid Epoch: 11 \t Loss: 0.009887676656246186 \t Accuracy: 77.19999694824219%\n",
            "{'fake_recall': 0.7516483516483516, 'fake_precision': 0.855, 'fake_f1': 0.7999999999999999, 'real_recall': 0.8033898305084746, 'real_precision': 0.6771428571428572, 'real_f1': 0.7348837209302327}\n",
            "model saves at 77.19999694824219 accuracy\n",
            "Train Epoch: 12 \t Loss: 0.0023063151905982878 \t Accuracy: 94.87445068359375%\n",
            "Valid Epoch: 12 \t Loss: 0.010983779867490133 \t Accuracy: 76.73332977294922%\n",
            "Train Epoch: 13 \t Loss: 0.002186432402699592 \t Accuracy: 95.27569580078125%\n",
            "Valid Epoch: 13 \t Loss: 0.010891670525074006 \t Accuracy: 77.0%\n",
            "Train Epoch: 14 \t Loss: 0.00208312276786484 \t Accuracy: 95.53456115722656%\n",
            "Valid Epoch: 14 \t Loss: 0.011202062795559565 \t Accuracy: 75.9000015258789%\n",
            "Train Epoch: 15 \t Loss: 0.001997188250060499 \t Accuracy: 95.5604476928711%\n",
            "Valid Epoch: 15 \t Loss: 0.011009537319342295 \t Accuracy: 77.0999984741211%\n",
            "Train Epoch: 16 \t Loss: 0.0019229986559509953 \t Accuracy: 95.94874572753906%\n",
            "Valid Epoch: 16 \t Loss: 0.011192273825407028 \t Accuracy: 75.86666870117188%\n",
            "Train Epoch: 17 \t Loss: 0.0019387205543853353 \t Accuracy: 95.884033203125%\n",
            "Valid Epoch: 17 \t Loss: 0.011125991210341454 \t Accuracy: 75.9000015258789%\n",
            "Train Epoch: 18 \t Loss: 0.0018716042170866672 \t Accuracy: 95.8193130493164%\n",
            "Valid Epoch: 18 \t Loss: 0.010402391781409582 \t Accuracy: 76.79999542236328%\n",
            "Train Epoch: 19 \t Loss: 0.00177480917570545 \t Accuracy: 96.10406494140625%\n",
            "Valid Epoch: 19 \t Loss: 0.011559898008902867 \t Accuracy: 75.9000015258789%\n",
            "Train Epoch: 20 \t Loss: 0.001799341459589095 \t Accuracy: 95.9616928100586%\n",
            "Valid Epoch: 20 \t Loss: 0.011653861696521441 \t Accuracy: 75.83333587646484%\n",
            "Train Epoch: 21 \t Loss: 0.0016539649410573984 \t Accuracy: 96.3758773803711%\n",
            "Valid Epoch: 21 \t Loss: 0.012219466269016266 \t Accuracy: 75.0999984741211%\n",
            "Train Epoch: 22 \t Loss: 0.001731171702795886 \t Accuracy: 96.18172454833984%\n",
            "Valid Epoch: 22 \t Loss: 0.011592883914709091 \t Accuracy: 76.76666259765625%\n",
            "Train Epoch: 23 \t Loss: 0.0016370563863803385 \t Accuracy: 96.28527069091797%\n",
            "Valid Epoch: 23 \t Loss: 0.010890242328246435 \t Accuracy: 76.69999694824219%\n",
            "Train Epoch: 24 \t Loss: 0.001609517772492275 \t Accuracy: 96.24644470214844%\n",
            "Valid Epoch: 24 \t Loss: 0.010820090234279633 \t Accuracy: 77.76666259765625%\n",
            "{'fake_recall': 0.7643059490084986, 'fake_precision': 0.843125, 'fake_f1': 0.8017830609212482, 'real_recall': 0.7967611336032389, 'real_precision': 0.7028571428571428, 'real_f1': 0.7468690702087286}\n",
            "model saves at 77.76666259765625 accuracy\n",
            "Train Epoch: 25 \t Loss: 0.0016559919871827681 \t Accuracy: 96.09112548828125%\n",
            "Valid Epoch: 25 \t Loss: 0.011324349224567414 \t Accuracy: 76.33333587646484%\n",
            "Train Epoch: 26 \t Loss: 0.001596998888517048 \t Accuracy: 96.27233123779297%\n",
            "Valid Epoch: 26 \t Loss: 0.012130963524182638 \t Accuracy: 75.23332977294922%\n",
            "Train Epoch: 27 \t Loss: 0.001577420283249745 \t Accuracy: 96.11701202392578%\n",
            "Valid Epoch: 27 \t Loss: 0.011413370907306672 \t Accuracy: 75.46666717529297%\n",
            "Train Epoch: 28 \t Loss: 0.0015350658032602436 \t Accuracy: 96.36293029785156%\n",
            "Valid Epoch: 28 \t Loss: 0.011464525143305461 \t Accuracy: 76.19999694824219%\n",
            "Train Epoch: 29 \t Loss: 0.0015767802235530798 \t Accuracy: 96.40176391601562%\n",
            "Valid Epoch: 29 \t Loss: 0.011378954907258352 \t Accuracy: 77.36666870117188%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}